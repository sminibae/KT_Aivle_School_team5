{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time, os, csv, random\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium.common.exceptions import TimeoutException\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the image\n",
    "url = 'https://m.media-amazon.com/images/I/81J2-2kYQqL._AC_SX425_.jpg'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful!s\n",
    "if response.status_code == 200:\n",
    "    # Open the image directly without saving\n",
    "    image = Image.open(BytesIO(response.content))\n",
    "    image.show()\n",
    "\n",
    "    # Alternatively, save the image to a file\n",
    "    with open('downloaded_image.jpg', 'wb') as file:\n",
    "        file.write(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Chrome options\n",
    "chrome_options = Options()\n",
    "# chrome_options.add_argument(\"--headless\")  # Run in headless mode (without opening a browser window)\n",
    "\n",
    "# Initialize the driver\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "# URL to scrape\n",
    "url = 'https://www.amazon.com/b/ref=s9_acss_bw_cg_FNNAV_2e1_w?node=14544497011&pf_rd_m=ATVPDKIKX0DER&pf_rd_s=merchandised-search-7&pf_rd_r=SFJVBA57GREEGHHVC0NH&pf_rd_t=101&pf_rd_p=d0c31dca-221f-4a26-9f62-a9e18ca9f354&pf_rd_i=1063306'\n",
    "\n",
    "# Open the URL\n",
    "driver.get(url)\n",
    "\n",
    "\n",
    "# Wait for the page to load\n",
    "time.sleep(5)  # Sleep for 5 seconds; adjust as needed\n",
    "\n",
    "# Now you can parse the page, interact with it, etc.\n",
    "# For example, to print the page source:\n",
    "print(driver.page_source)\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sofas\n",
    "Sectional_Sofas = 'https://www.amazon.com/s?bbn=3733551&rh=n%3A1055398%2Cn%3A%211063498%2Cn%3A1063306%2Cn%3A1063318%2Cn%3A3733551%2Cp_n_feature_two_browse-bin%3A3248836011&dc&fst=as%3Aoff&pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_r=0X2CMRE53H2HSQR09781&pf_rd_s=merchandised-search-2&pf_rd_t=101&qid=1528841766&rnid=3248834011&ref=s9_acss_bw_cg_HarSofa_2a1_w'\n",
    "Sleeper_Sofas = 'https://www.amazon.com/s?i=garden&bbn=3733551&rh=n%3A1055398%2Cn%3A%211063498%2Cn%3A1063306%2Cn%3A1063318%2Cn%3A3733551%2Cp_n_feature_two_browse-bin%3A3248838011&pf_rd_i=3733551&pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=3009ccc5-2584-454e-b732-ae8e525543fe&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_r=0X2CMRE53H2HSQR09781&pf_rd_r=79FTK5EJP86JTXQETKYX&pf_rd_s=merchandised-search-2&pf_rd_s=merchandised-search-2&pf_rd_t=101&pf_rd_t=101&ref=s9_acss_bw_cg_HarSofa_2b1_w'\n",
    "Reclining_Sofas = 'https://www.amazon.com/s?i=garden&bbn=3733551&rh=n%3A1055398%2Cn%3A%211063498%2Cn%3A1063306%2Cn%3A1063318%2Cn%3A3733551%2Cp_n_feature_two_browse-bin%3A12012870011&pf_rd_i=3733551&pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=3009ccc5-2584-454e-b732-ae8e525543fe&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_r=0X2CMRE53H2HSQR09781&pf_rd_r=79FTK5EJP86JTXQETKYX&pf_rd_s=merchandised-search-2&pf_rd_s=merchandised-search-2&pf_rd_t=101&pf_rd_t=101&ref=s9_acss_bw_cg_HarSofa_2c1_w'\n",
    "LoveSeats = 'https://www.amazon.com/s?i=garden&bbn=3733551&rh=n%3A1055398%2Cn%3A%211063498%2Cn%3A1063306%2Cn%3A1063318%2Cn%3A3733551%2Cp_n_feature_two_browse-bin%3A3248835011&pf_rd_i=3733551&pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=3009ccc5-2584-454e-b732-ae8e525543fe&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_r=79FTK5EJP86JTXQETKYX&pf_rd_r=QZ6K0G39A8CCYDMBWJRD&pf_rd_s=merchandised-search-2&pf_rd_s=merchandised-search-2&pf_rd_t=101&pf_rd_t=101&ref=s9_acss_bw_cg_HarSofa_2d1_w'\n",
    "Futons = 'https://www.amazon.com/Futons/b/ref=s9_acss_bw_cg_SofaType_1f1_w/ref=s9_acss_bw_cg_HarSofa_3a1_w?ie=UTF8&node=13753041&pf_rd_m=ATVPDKIKX0DER&pf_rd_s=merchandised-search-2&pf_rd_r=79FTK5EJP86JTXQETKYX&pf_rd_t=101&pf_rd_p=3009ccc5-2584-454e-b732-ae8e525543fe&pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_s=merchandised-search-2&pf_rd_r=QZ6K0G39A8CCYDMBWJRD&pf_rd_t=101&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_i=3733551'\n",
    "Settles = 'https://www.amazon.com/s?i=garden&bbn=3733551&rh=n%3A1055398%2Cn%3A%211063498%2Cn%3A1063306%2Cn%3A1063318%2Cn%3A3733551%2Cp_n_feature_two_browse-bin%3A3248837011&pf_rd_i=3733551&pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=3009ccc5-2584-454e-b732-ae8e525543fe&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_r=79FTK5EJP86JTXQETKYX&pf_rd_r=QZ6K0G39A8CCYDMBWJRD&pf_rd_s=merchandised-search-2&pf_rd_s=merchandised-search-2&pf_rd_t=101&pf_rd_t=101&ref=s9_acss_bw_cg_HarSofa_3b1_w'\n",
    "Convertibles = 'https://www.amazon.com/s?bbn=3733551&rh=n%3A1055398%2Cn%3A%211063498%2Cn%3A1063306%2Cn%3A1063318%2Cn%3A3733551%2Cp_n_feature_two_browse-bin%3A12012869011&dc&fst=as%3Aoff&pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_r=QZ6K0G39A8CCYDMBWJRD&pf_rd_s=merchandised-search-2&pf_rd_t=101&qid=1528765569&rnid=3248834011&ref=s9_acss_bw_cg_HarSofa_3c1_w'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get html of 113 pages of Sectional Sofas\n",
    "# Set up Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode (without opening a browser window)\n",
    "chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\")\n",
    "\n",
    "# Initialize the driver\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "# Get the page source\n",
    "test_product_url = 'https://www.amazon.com/Acanva-Convertible-Sectional-L-Shaped-1-Seater/dp/B0BLRPVDV3/ref=sr_1_28?fst=as%3Aoff&pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_r=0X2CMRE53H2HSQR09781&pf_rd_s=merchandised-search-2&pf_rd_t=101&qid=1702443247&refinements=p_n_feature_two_browse-bin%3A3248836011&rnid=3248834011&s=home-garden&sr=1-28'\n",
    "test_product_url2 = 'https://www.amazon.com/Ashley-Furniture-Signature-Design-Contemporary/dp/B072VJWCMR/ref=sr_1_26?fst=as%3Aoff&pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_r=0X2CMRE53H2HSQR09781&pf_rd_s=merchandised-search-2&pf_rd_t=101&qid=1702443247&refinements=p_n_feature_two_browse-bin%3A3248836011&rnid=3248834011&s=home-garden&sr=1-26'\n",
    "test_product_url3 = 'https://www.amazon.com/WhioCrest-81-5-Inch-Oversized-Sectional-Apartment/dp/B0CP9DWH3V/ref=sr_1_50?fst=as%3Aoff&pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_r=0X2CMRE53H2HSQR09781&pf_rd_s=merchandised-search-2&pf_rd_t=101&qid=1702447244&refinements=p_n_feature_two_browse-bin%3A3248836011&rnid=3248834011&s=home-garden&sr=1-50'\n",
    "test_product_url4 = 'https://www.amazon.com/KENVA-Upholstered-L-Shaped-Pillows-Included/dp/B0CP4R66CX/ref=sr_1_56?fst=as%3Aoff&pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_r=0X2CMRE53H2HSQR09781&pf_rd_s=merchandised-search-2&pf_rd_t=101&qid=1702447446&refinements=p_n_feature_two_browse-bin%3A3248836011&rnid=3248834011&s=home-garden&sr=1-56'\n",
    "test_product_url5 = 'https://www.amazon.com/Flash-Furniture-Living-Room-Grouping-Sofa/dp/B0BCX2TRJ3/ref=sr_1_25?pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=3009ccc5-2584-454e-b732-ae8e525543fe&pf_rd_r=0X2CMRE53H2HSQR09781&pf_rd_s=merchandised-search-2&pf_rd_t=101&qid=1702472990&refinements=p_n_feature_two_browse-bin%3A12012870011&s=home-garden&sr=1-25&th=1'\n",
    "\n",
    "page1_url = 'https://www.amazon.com/s?i=garden&bbn=3733551&rh=n%3A1055398%2Cn%3A1063498%2Cn%3A1063306%2Cn%3A1063318%2Cn%3A3733551%2Cp_n_feature_two_browse-bin%3A3248836011&dc&fst=as%3Aoff&pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_r=0X2CMRE53H2HSQR09781&pf_rd_s=merchandised-search-2&pf_rd_t=101&qid=1702448253&rnid=3248834011&ref=sr_pg_1'\n",
    "page3_url = 'https://www.amazon.com/s?i=garden&bbn=3733551&rh=n%3A1055398%2Cn%3A1063498%2Cn%3A1063306%2Cn%3A1063318%2Cn%3A3733551%2Cp_n_feature_two_browse-bin%3A3248836011&dc&page=3&fst=as%3Aoff&pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_r=0X2CMRE53H2HSQR09781&pf_rd_s=merchandised-search-2&pf_rd_t=101&qid=1702448136&rnid=3248834011&ref=sr_pg_3'\n",
    "\n",
    "driver.get(page3_url)\n",
    "html_content = driver.page_source\n",
    "\n",
    "# Write the HTML content to a file\n",
    "with open('page3.html', 'w', encoding='utf-8') as f:\n",
    "    f.write(html_content)\n",
    "    \n",
    "# Get the p\n",
    "# Close the driver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Page 들 URL 가져오기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# File path to your HTML file\n",
    "file_path = 'page3.html'\n",
    "\n",
    "# Open and read the HTML file\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    html_content = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.amazon.com/s?i=garden&bbn=3733551&rh=n%3A1055398%2Cn%3A1063498%2Cn%3A1063306%2Cn%3A1063318%2Cn%3A3733551%2Cp_n_feature_two_browse-bin%3A3248836011&dc&page=4&fst=as%3Aoff&pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_r=0X2CMRE53H2HSQR09781&pf_rd_s=merchandised-search-2&pf_rd_t=101&qid=1702448459&rnid=3248834011&ref=sr_pg_3\n"
     ]
    }
   ],
   "source": [
    "# Initialize BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Find all <a> tags and filter with a lambda function for the one with 'Go to next page' in aria-label\n",
    "next_page_tags = soup.find_all(lambda tag: tag.name == 'a' and 'aria-label' in tag.attrs and 'Go to next page' in tag['aria-label'])\n",
    "\n",
    "# Extract the href attribute from the first matching tag and prepend with 'https://www.amazon.com'\n",
    "next_page_url = 'https://www.amazon.com' + next_page_tags[0]['href'] if next_page_tags else 'Next page URL not found'\n",
    "\n",
    "# Print the URL\n",
    "print(next_page_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# page3 에서 각 제품 링크 뽑기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# File path to your HTML file\n",
    "file_path = 'page3.html'\n",
    "\n",
    "# Open and read the HTML file\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    html_content = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 https://www.amazon.com/sspa/click?ie=UTF8&spc=MTo0NTg1NDc0NjYxMjcxNzEzOjE3MDI2MDc1Mjg6c3BfYXRmX2Jyb3dzZToyMDAxNzMxMjMzODg4OTg6OjA6Og&url=%2FCasa-Andrea-Milano-U-Shape-Sectional%2Fdp%2FB0BG79J5QY%2Fref%3Dsr_1_49_sspa%3Ffst%3Das%253Aoff%26pf_rd_i%3D3733551%26pf_rd_m%3DATVPDKIKX0DER%26pf_rd_p%3D877c0809-fc53-42a6-91f2-7c8f50e6e9be%26pf_rd_r%3D0X2CMRE53H2HSQR09781%26pf_rd_s%3Dmerchandised-search-2%26pf_rd_t%3D101%26qid%3D1702607528%26refinements%3Dp_n_feature_two_browse-bin%253A3248836011%26rnid%3D3248834011%26s%3Dhome-garden%26sr%3D1-49-spons%26sp_csd%3Dd2lkZ2V0TmFtZT1zcF9hdGZfYnJvd3Nl%26psc%3D1\n",
      "3 https://www.amazon.com/sspa/click?ie=UTF8&spc=MTo0NTg1NDc0NjYxMjcxNzEzOjE3MDI2MDc1Mjg6c3BfYXRmX2Jyb3dzZTozMDAwNTQ1Mjg1MDM4MDI6OjA6Og&url=%2FHONBAY-Modular-Sectional-Storage-Sleeper%2Fdp%2FB0BZ4B2VVQ%2Fref%3Dsr_1_50_sspa%3Ffst%3Das%253Aoff%26pf_rd_i%3D3733551%26pf_rd_m%3DATVPDKIKX0DER%26pf_rd_p%3D877c0809-fc53-42a6-91f2-7c8f50e6e9be%26pf_rd_r%3D0X2CMRE53H2HSQR09781%26pf_rd_s%3Dmerchandised-search-2%26pf_rd_t%3D101%26qid%3D1702607528%26refinements%3Dp_n_feature_two_browse-bin%253A3248836011%26rnid%3D3248834011%26s%3Dhome-garden%26sr%3D1-50-spons%26sp_csd%3Dd2lkZ2V0TmFtZT1zcF9hdGZfYnJvd3Nl%26psc%3D1\n",
      "4 https://www.amazon.com/sspa/click?ie=UTF8&spc=MTo0NTg1NDc0NjYxMjcxNzEzOjE3MDI2MDc1Mjg6c3BfYXRmX2Jyb3dzZTozMDAxMDMyMDIxMzEwMDI6OjA6Og&url=%2FHONBAY-Convertible-Sectional-Reversible-Apartment%2Fdp%2FB0CPWBCPNJ%2Fref%3Dsr_1_51_sspa%3Ffst%3Das%253Aoff%26pf_rd_i%3D3733551%26pf_rd_m%3DATVPDKIKX0DER%26pf_rd_p%3D877c0809-fc53-42a6-91f2-7c8f50e6e9be%26pf_rd_r%3D0X2CMRE53H2HSQR09781%26pf_rd_s%3Dmerchandised-search-2%26pf_rd_t%3D101%26qid%3D1702607528%26refinements%3Dp_n_feature_two_browse-bin%253A3248836011%26rnid%3D3248834011%26s%3Dhome-garden%26sr%3D1-51-spons%26sp_csd%3Dd2lkZ2V0TmFtZT1zcF9hdGZfYnJvd3Nl%26psc%3D1\n",
      "5 https://www.amazon.com/Karen-Traditional-Chesterfield-Loveseat-Beige/dp/B07G7W33D1/ref=sr_1_52?fst=as%3Aoff&pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_r=0X2CMRE53H2HSQR09781&pf_rd_s=merchandised-search-2&pf_rd_t=101&qid=1702607528&refinements=p_n_feature_two_browse-bin%3A3248836011&rnid=3248834011&s=home-garden&sr=1-52\n",
      "6 https://www.amazon.com/HONBAY-Convertible-Sectional-Shaped-Chaises/dp/B0C1NV654W/ref=sr_1_53?fst=as%3Aoff&pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_r=0X2CMRE53H2HSQR09781&pf_rd_s=merchandised-search-2&pf_rd_t=101&qid=1702607528&refinements=p_n_feature_two_browse-bin%3A3248836011&rnid=3248834011&s=home-garden&sr=1-53\n",
      "7 https://www.amazon.com/HONBAY-Oversized-Sofa-Reversible-Modular/dp/B09Z6S91ZZ/ref=sr_1_54?fst=as%3Aoff&pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_r=0X2CMRE53H2HSQR09781&pf_rd_s=merchandised-search-2&pf_rd_t=101&qid=1702607528&refinements=p_n_feature_two_browse-bin%3A3248836011&rnid=3248834011&s=home-garden&sr=1-54\n",
      "9 https://www.amazon.com/JAMFLY-Convertible-Sectional-Reversible-Apartment/dp/B09NFT7WTF/ref=sr_1_55?fst=as%3Aoff&pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_r=0X2CMRE53H2HSQR09781&pf_rd_s=merchandised-search-2&pf_rd_t=101&qid=1702607528&refinements=p_n_feature_two_browse-bin%3A3248836011&rnid=3248834011&s=home-garden&sr=1-55\n",
      "10 https://www.amazon.com/HONBAY-Convertible-Sectional-Chaise-Holders/dp/B0BG4N24J7/ref=sr_1_56?fst=as%3Aoff&pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_r=0X2CMRE53H2HSQR09781&pf_rd_s=merchandised-search-2&pf_rd_t=101&qid=1702607528&refinements=p_n_feature_two_browse-bin%3A3248836011&rnid=3248834011&s=home-garden&sr=1-56\n",
      "11 https://www.amazon.com/DURASPACE-Velvet-Sectional-Convertible-Folding/dp/B0CB3L8431/ref=sr_1_57?fst=as%3Aoff&pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_r=0X2CMRE53H2HSQR09781&pf_rd_s=merchandised-search-2&pf_rd_t=101&qid=1702607528&refinements=p_n_feature_two_browse-bin%3A3248836011&rnid=3248834011&s=home-garden&sr=1-57\n",
      "12 https://www.amazon.com/Casa-Andrea-Milano-L-Shape-Sectional/dp/B0BZQWS4PV/ref=sr_1_58?fst=as%3Aoff&pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_r=0X2CMRE53H2HSQR09781&pf_rd_s=merchandised-search-2&pf_rd_t=101&qid=1702607528&refinements=p_n_feature_two_browse-bin%3A3248836011&rnid=3248834011&s=home-garden&sr=1-58\n",
      "13 https://www.amazon.com/Vivaadecor-3Ottomans-Washable-Sectional-Convertible/dp/B0CNDCBTDB/ref=sr_1_59?fst=as%3Aoff&pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_r=0X2CMRE53H2HSQR09781&pf_rd_s=merchandised-search-2&pf_rd_t=101&qid=1702607528&refinements=p_n_feature_two_browse-bin%3A3248836011&rnid=3248834011&s=home-garden&sr=1-59\n",
      "14 https://www.amazon.com/Nolany-Convertible-Sectional-L-Shape-Reversible/dp/B0BXDKMX4W/ref=sr_1_60?fst=as%3Aoff&pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_r=0X2CMRE53H2HSQR09781&pf_rd_s=merchandised-search-2&pf_rd_t=101&qid=1702607528&refinements=p_n_feature_two_browse-bin%3A3248836011&rnid=3248834011&s=home-garden&sr=1-60\n",
      "15 https://www.amazon.com/Belffin-Convertible-Sectional-Velvet-Reversible/dp/B0CJDJSWLH/ref=sr_1_61?fst=as%3Aoff&pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_r=0X2CMRE53H2HSQR09781&pf_rd_s=merchandised-search-2&pf_rd_t=101&qid=1702607528&refinements=p_n_feature_two_browse-bin%3A3248836011&rnid=3248834011&s=home-garden&sr=1-61\n",
      "16 https://www.amazon.com/HONBAY-Convertible-Sectional-Reversible-Apartment/dp/B0BSV3KDJ5/ref=sr_1_62?fst=as%3Aoff&pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_r=0X2CMRE53H2HSQR09781&pf_rd_s=merchandised-search-2&pf_rd_t=101&qid=1702607528&refinements=p_n_feature_two_browse-bin%3A3248836011&rnid=3248834011&s=home-garden&sr=1-62\n",
      "17 https://www.amazon.com/PaPaJet-Convertible-Sleeper-Chenille-Sectional/dp/B0C14LZ4CB/ref=sr_1_63?fst=as%3Aoff&pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_r=0X2CMRE53H2HSQR09781&pf_rd_s=merchandised-search-2&pf_rd_t=101&qid=1702607528&refinements=p_n_feature_two_browse-bin%3A3248836011&rnid=3248834011&s=home-garden&sr=1-63\n",
      "18 https://www.amazon.com/sspa/click?ie=UTF8&spc=MTo0NTg1NDc0NjYxMjcxNzEzOjE3MDI2MDc1Mjg6c3BfbXRmX2Jyb3dzZTozMDAwNDE3ODIwODE1MDI6OjA6Og&url=%2FCHITA-Oversized-Sectional-Reversible-Ottamans%2Fdp%2FB0C3CWKF55%2Fref%3Dsr_1_64_sspa%3Ffst%3Das%253Aoff%26pf_rd_i%3D3733551%26pf_rd_m%3DATVPDKIKX0DER%26pf_rd_p%3D877c0809-fc53-42a6-91f2-7c8f50e6e9be%26pf_rd_r%3D0X2CMRE53H2HSQR09781%26pf_rd_s%3Dmerchandised-search-2%26pf_rd_t%3D101%26qid%3D1702607528%26refinements%3Dp_n_feature_two_browse-bin%253A3248836011%26rnid%3D3248834011%26s%3Dhome-garden%26sr%3D1-64-spons%26sp_csd%3Dd2lkZ2V0TmFtZT1zcF9tdGZfYnJvd3Nl%26psc%3D1\n",
      "19 https://www.amazon.com/sspa/click?ie=UTF8&spc=MTo0NTg1NDc0NjYxMjcxNzEzOjE3MDI2MDc1Mjg6c3BfbXRmX2Jyb3dzZTozMDAxMDI3MTU3MTY2MDI6OjA6Og&url=%2FSIENWIEY-Sectional-Convertible-Reversible-Apartment%2Fdp%2FB0CNK7MH7W%2Fref%3Dsr_1_65_sspa%3Ffst%3Das%253Aoff%26pf_rd_i%3D3733551%26pf_rd_m%3DATVPDKIKX0DER%26pf_rd_p%3D877c0809-fc53-42a6-91f2-7c8f50e6e9be%26pf_rd_r%3D0X2CMRE53H2HSQR09781%26pf_rd_s%3Dmerchandised-search-2%26pf_rd_t%3D101%26qid%3D1702607528%26refinements%3Dp_n_feature_two_browse-bin%253A3248836011%26rnid%3D3248834011%26s%3Dhome-garden%26sr%3D1-65-spons%26sp_csd%3Dd2lkZ2V0TmFtZT1zcF9tdGZfYnJvd3Nl%26psc%3D1\n",
      "20 https://www.amazon.com/sspa/click?ie=UTF8&spc=MTo0NTg1NDc0NjYxMjcxNzEzOjE3MDI2MDc1Mjg6c3BfbXRmX2Jyb3dzZTozMDAwNzUyMzQ3OTM3MDI6OjA6Og&url=%2FHONBAY-Modular-Oversized-Sectional-Reversible%2Fdp%2FB0CLHBFS7F%2Fref%3Dsr_1_66_sspa%3Ffst%3Das%253Aoff%26pf_rd_i%3D3733551%26pf_rd_m%3DATVPDKIKX0DER%26pf_rd_p%3D877c0809-fc53-42a6-91f2-7c8f50e6e9be%26pf_rd_r%3D0X2CMRE53H2HSQR09781%26pf_rd_s%3Dmerchandised-search-2%26pf_rd_t%3D101%26qid%3D1702607528%26refinements%3Dp_n_feature_two_browse-bin%253A3248836011%26rnid%3D3248834011%26s%3Dhome-garden%26sr%3D1-66-spons%26sp_csd%3Dd2lkZ2V0TmFtZT1zcF9tdGZfYnJvd3Nl%26psc%3D1\n",
      "21 https://www.amazon.com/Vonanda-Sectional-Convertible-Reversible-Apartment/dp/B0C286RTT9/ref=sr_1_67?fst=as%3Aoff&pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_r=0X2CMRE53H2HSQR09781&pf_rd_s=merchandised-search-2&pf_rd_t=101&qid=1702607528&refinements=p_n_feature_two_browse-bin%3A3248836011&rnid=3248834011&s=home-garden&sr=1-67\n",
      "22 https://www.amazon.com/Livavege-Convertible-Sleeper-Sectional-Sofa/dp/B0CNRFTTSV/ref=sr_1_68?fst=as%3Aoff&pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_r=0X2CMRE53H2HSQR09781&pf_rd_s=merchandised-search-2&pf_rd_t=101&qid=1702607528&refinements=p_n_feature_two_browse-bin%3A3248836011&rnid=3248834011&s=home-garden&sr=1-68\n",
      "23 https://www.amazon.com/ATMUTE-Sectional-Sofa-Couch-Convertible/dp/B0CLRSQVYX/ref=sr_1_69?fst=as%3Aoff&pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_r=0X2CMRE53H2HSQR09781&pf_rd_s=merchandised-search-2&pf_rd_t=101&qid=1702607528&refinements=p_n_feature_two_browse-bin%3A3248836011&rnid=3248834011&s=home-garden&sr=1-69\n",
      "24 https://www.amazon.com/ZeeFu-Convertible-Sectional-Upholstered-Reversible/dp/B0CCS742L9/ref=sr_1_70?fst=as%3Aoff&pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_r=0X2CMRE53H2HSQR09781&pf_rd_s=merchandised-search-2&pf_rd_t=101&qid=1702607528&refinements=p_n_feature_two_browse-bin%3A3248836011&rnid=3248834011&s=home-garden&sr=1-70\n",
      "25 https://www.amazon.com/Devion-Furniture-Contemporary-Reversible-Sectional/dp/B0B5F18918/ref=sr_1_71?fst=as%3Aoff&pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_r=0X2CMRE53H2HSQR09781&pf_rd_s=merchandised-search-2&pf_rd_t=101&qid=1702607528&refinements=p_n_feature_two_browse-bin%3A3248836011&rnid=3248834011&s=home-garden&sr=1-71\n",
      "26 https://www.amazon.com/Tornama-Convertible-Sectional-Reversible-Apartment/dp/B0CFKQYSJK/ref=sr_1_72?fst=as%3Aoff&pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_r=0X2CMRE53H2HSQR09781&pf_rd_s=merchandised-search-2&pf_rd_t=101&qid=1702607528&refinements=p_n_feature_two_browse-bin%3A3248836011&rnid=3248834011&s=home-garden&sr=1-72\n",
      "27 https://www.amazon.com/Belffin-Modular-Sectional-Sleeper-Storage/dp/B0BPHHGQF5/ref=sr_1_73?fst=as%3Aoff&pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_r=0X2CMRE53H2HSQR09781&pf_rd_s=merchandised-search-2&pf_rd_t=101&qid=1702607528&refinements=p_n_feature_two_browse-bin%3A3248836011&rnid=3248834011&s=home-garden&sr=1-73\n",
      "28 https://www.amazon.com/Belffin-Shaped-Convertible-Sectional-Reversible/dp/B0CJ2B3MC9/ref=sr_1_74?fst=as%3Aoff&pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_r=0X2CMRE53H2HSQR09781&pf_rd_s=merchandised-search-2&pf_rd_t=101&qid=1702607528&refinements=p_n_feature_two_browse-bin%3A3248836011&rnid=3248834011&s=home-garden&sr=1-74\n",
      "29 https://www.amazon.com/Belffin-Modular-Sectional-Oversized-Reversible/dp/B09MVWC9VF/ref=sr_1_75?fst=as%3Aoff&pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_r=0X2CMRE53H2HSQR09781&pf_rd_s=merchandised-search-2&pf_rd_t=101&qid=1702607528&refinements=p_n_feature_two_browse-bin%3A3248836011&rnid=3248834011&s=home-garden&sr=1-75\n",
      "30 https://www.amazon.com/VanAcc-Convertible-Sleeper-Chenille-Sectional/dp/B0CDW8GW5L/ref=sr_1_76?fst=as%3Aoff&pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_r=0X2CMRE53H2HSQR09781&pf_rd_s=merchandised-search-2&pf_rd_t=101&qid=1702607528&refinements=p_n_feature_two_browse-bin%3A3248836011&rnid=3248834011&s=home-garden&sr=1-76\n",
      "31 https://www.amazon.com/Youmumeub-Convertible-Sectional-Reversible-Apartment/dp/B0C4HBM87D/ref=sr_1_77?fst=as%3Aoff&pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_r=0X2CMRE53H2HSQR09781&pf_rd_s=merchandised-search-2&pf_rd_t=101&qid=1702607528&refinements=p_n_feature_two_browse-bin%3A3248836011&rnid=3248834011&s=home-garden&sr=1-77\n",
      "32 https://www.amazon.com/Karl-home-Sectional-Upholstered-Creamy-White/dp/B0BW8VWWC9/ref=sr_1_78?fst=as%3Aoff&pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_r=0X2CMRE53H2HSQR09781&pf_rd_s=merchandised-search-2&pf_rd_t=101&qid=1702607528&refinements=p_n_feature_two_browse-bin%3A3248836011&rnid=3248834011&s=home-garden&sr=1-78\n",
      "33 https://www.amazon.com/sspa/click?ie=UTF8&spc=MTo0NTg1NDc0NjYxMjcxNzEzOjE3MDI2MDc1Mjg6c3BfYnRmX2Jyb3dzZTozMDAwOTIxNzUyNTI5MDI6OjA6Og&url=%2FVECELO-Convertible-Sectional-Reversible-Apartment%2Fdp%2FB0CMQNRG57%2Fref%3Dsr_1_79_sspa%3Ffst%3Das%253Aoff%26pf_rd_i%3D3733551%26pf_rd_m%3DATVPDKIKX0DER%26pf_rd_p%3D877c0809-fc53-42a6-91f2-7c8f50e6e9be%26pf_rd_r%3D0X2CMRE53H2HSQR09781%26pf_rd_s%3Dmerchandised-search-2%26pf_rd_t%3D101%26qid%3D1702607528%26refinements%3Dp_n_feature_two_browse-bin%253A3248836011%26rnid%3D3248834011%26s%3Dhome-garden%26sr%3D1-79-spons%26sp_csd%3Dd2lkZ2V0TmFtZT1zcF9idGZfYnJvd3Nl%26psc%3D1\n",
      "34 https://www.amazon.com/sspa/click?ie=UTF8&spc=MTo0NTg1NDc0NjYxMjcxNzEzOjE3MDI2MDc1Mjg6c3BfYnRmX2Jyb3dzZTozMDAxMDE0MjA1MDQzMDI6OjA6Og&url=%2FKIMOHOME-Sectional-Couches-Chenille-L-Shape%2Fdp%2FB0CKSHJRYM%2Fref%3Dsr_1_80_sspa%3Ffst%3Das%253Aoff%26pf_rd_i%3D3733551%26pf_rd_m%3DATVPDKIKX0DER%26pf_rd_p%3D877c0809-fc53-42a6-91f2-7c8f50e6e9be%26pf_rd_r%3D0X2CMRE53H2HSQR09781%26pf_rd_s%3Dmerchandised-search-2%26pf_rd_t%3D101%26qid%3D1702607528%26refinements%3Dp_n_feature_two_browse-bin%253A3248836011%26rnid%3D3248834011%26s%3Dhome-garden%26sr%3D1-80-spons%26sp_csd%3Dd2lkZ2V0TmFtZT1zcF9idGZfYnJvd3Nl%26psc%3D1\n",
      "35 https://www.amazon.com/sspa/click?ie=UTF8&spc=MTo0NTg1NDc0NjYxMjcxNzEzOjE3MDI2MDc1Mjg6c3BfYnRmX2Jyb3dzZTozMDAwOTc1NDI0NTcxMDI6OjA6Og&url=%2Fwirrytor-Convertible-Sectional-Upholstered-Apartment%2Fdp%2FB0CP3S5BFJ%2Fref%3Dsr_1_81_sspa%3Ffst%3Das%253Aoff%26pf_rd_i%3D3733551%26pf_rd_m%3DATVPDKIKX0DER%26pf_rd_p%3D877c0809-fc53-42a6-91f2-7c8f50e6e9be%26pf_rd_r%3D0X2CMRE53H2HSQR09781%26pf_rd_s%3Dmerchandised-search-2%26pf_rd_t%3D101%26qid%3D1702607528%26refinements%3Dp_n_feature_two_browse-bin%253A3248836011%26rnid%3D3248834011%26s%3Dhome-garden%26sr%3D1-81-spons%26sp_csd%3Dd2lkZ2V0TmFtZT1zcF9idGZfYnJvd3Nl%26psc%3D1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Find the <a> tag with the specified class\n",
    "# Initialize BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Find all divs with the 'data-index' attribute\n",
    "divs_with_data_index = soup.find_all('div', attrs={'data-index': True})\n",
    "\n",
    "# Initialize a dictionary to hold the URLs with their corresponding data-index\n",
    "index_url_dict = {}\n",
    "\n",
    "# Loop through the found divs and extract the href attributes\n",
    "for div in divs_with_data_index:\n",
    "    data_index = div['data-index']\n",
    "    a_tag = div.find('a', class_='a-link-normal')\n",
    "    if a_tag and 'href' in a_tag.attrs:\n",
    "        full_url = 'https://www.amazon.com' + a_tag['href']\n",
    "        if 'help/customer' in full_url:\n",
    "                continue\n",
    "        index_url_dict[data_index] = full_url\n",
    "\n",
    "# Print the dictionary of URLs\n",
    "for key, item in index_url_dict.items():\n",
    "    print(key, item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 각 제품 페이지에서 정보 뽑기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# File path to your HTML file\n",
    "file_path = 'product5.html'\n",
    "\n",
    "# Open and read the HTML file\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    html_content = file.read()\n",
    "    \n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://m.media-amazon.com/images/W/MEDIAX_792452-T2/images/I/51R08TyH0yL._AC_SL1500_.jpg\n"
     ]
    }
   ],
   "source": [
    "# Select the img tags within elements with ID 'imgTagWrapperId' or class 'imgTagWrapper'\n",
    "selected_imgs = soup.select('#imgTagWrapperId img, .imgTagWrapper img')\n",
    "\n",
    "# Extract the image URLs\n",
    "for img in selected_imgs:\n",
    "    image_url = img.get('data-old-hires', img.get('src'))  # Default to src if data-old-hires is not present\n",
    "    print(image_url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(image_url, name, title):\n",
    "    try:\n",
    "        # Send a GET request to the image URL\n",
    "        response = requests.get(image_url)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "\n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(f'imgs/{name}', exist_ok=True)\n",
    "\n",
    "        # Save image file\n",
    "        with open(f'imgs/{name}/{title[:200]}.png', 'wb') as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "        # Optional: Convert to PNG if necessary\n",
    "        if not image_url.lower().endswith('.png'):\n",
    "            img = Image.open(BytesIO(response.content))\n",
    "            img.save(f'imgs/{name}/{title[:200]}.png', 'PNG')\n",
    "            \n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error downloading image: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving image: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Home & Kitchen', 'Furniture', 'Living Room Furniture', 'Sofas & Couches']\n"
     ]
    }
   ],
   "source": [
    "import html\n",
    "\n",
    "# Find all <a> tags within the breadcrumb list\n",
    "breadcrumb_links = soup.select('#wayfinding-breadcrumbs_feature_div ul a')\n",
    "\n",
    "# Extract the text from each link, decode HTML entities, and add to list\n",
    "breadcrumbs = [html.unescape(link.get_text().strip()) for link in breadcrumb_links]\n",
    "\n",
    "print(breadcrumbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flash Furniture Carter Premium Tufted Split Back Sofa Futon -Navy Velvet Upholstery - Solid Wood Legs - Convertible Sleeper Couch for Small Spaces\n"
     ]
    }
   ],
   "source": [
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Find the span with id 'productTitle' and extract text\n",
    "product_title = soup.find('span', {'id': 'productTitle'}).get_text(strip=True)\n",
    "\n",
    "print(product_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$60199\n"
     ]
    }
   ],
   "source": [
    "# price\n",
    "\n",
    "# Find the span tags for the whole and fractional parts of the price\n",
    "price_whole_part = soup.find('span', class_='a-price-whole')\n",
    "price_fraction_part = soup.find('span', class_='a-price-fraction')\n",
    "\n",
    "# Extract and concatenate the price\n",
    "price = f'${price_whole_part.get_text(strip=True)}{price_fraction_part.get_text(strip=True)}' if price_whole_part and price_fraction_part else 'Price not Found'\n",
    "\n",
    "# Print the price\n",
    "print(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Brand': 'Signature Design by Ashley', 'Assembly Required': 'No', 'Seat Depth': '22 inches', 'Seat Height': '19 Inches', 'Item Weight': '111 Pounds', 'Type': 'Sectional', 'Color': 'Java', 'Upholstery Fabric Type': 'Polyester', 'Room Type': 'Living Room', 'Arm Style': 'Sock'}\n"
     ]
    }
   ],
   "source": [
    "# Find the table\n",
    "table = soup.find('table', class_='a-normal a-spacing-micro')\n",
    "\n",
    "# Initialize an empty dictionary to store the data\n",
    "product_info = {}\n",
    "\n",
    "# Loop through each row in the table\n",
    "for row in table.find_all('tr'):\n",
    "    # Extract columns: key and value\n",
    "    columns = row.find_all('td')\n",
    "    if len(columns) == 2:  # Ensure that there are exactly 2 columns\n",
    "        key = columns[0].get_text(strip=True)\n",
    "        value = columns[1].get_text(strip=True)\n",
    "        product_info[key] = value\n",
    "        \n",
    "print(product_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Style': 'Contemporary', 'Shape': 'L-Shape', 'Frame Material': 'Wood', 'Material': 'Wood'}\n"
     ]
    }
   ],
   "source": [
    "# Find all the relevant tables\n",
    "tables = soup.find_all('table', class_='a-normal')\n",
    "\n",
    "# Initialize an empty dictionary to store the data\n",
    "product_features = {}\n",
    "\n",
    "# Loop through each table\n",
    "for table in tables:\n",
    "    rows = table.find_all('tr')\n",
    "    for row in rows:\n",
    "        # Extract the feature name\n",
    "        feature_name_element = row.find('span', class_='a-text-bold')\n",
    "        if feature_name_element:\n",
    "            feature_name = feature_name_element.get_text(strip=True)\n",
    "            # Extract the feature value\n",
    "            feature_values = row.find_all('span', class_='a-size-base handle-overflow')\n",
    "            if feature_values:\n",
    "                feature_value = feature_values[-1].get_text(strip=True)\n",
    "                product_features[feature_name] = feature_value\n",
    "\n",
    "# Print the resulting dictionary\n",
    "print(product_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RIGHT ARM FACING SOFA ONLY: Oversized scale and feel-good microfiber upholstery make it all too easy to kick back and relax into this sofa. Additional sectional pieces sold separately SPACIOUS COMFORT: Wrap-around upholstery gracing the corner-block frame has a sleek, seamless aesthetic, Narrow feet in a dark faux wood finish keep the look clean and crisp EARTH-TONE UPHOLSTERY: Sporting a hint of texture, this sofa's upholstery is an earthy-elegant brown is soothing and practical, Polyester/nylon microfiber upholstery is made for everyday living MINOR ASSEMBLY: Once each element of the sofa set is fastened together with the resilient universal connectors, a full sectional is ready for instant enjoyment. DIRECT FROM THE MANUFACTURER: Ashley Furniture goes the extra mile to package, protect and deliver your purchase in a timely manner BUY WITH CONFIDENCE: Designed and manufactured by Ashley Furniture Industries. The trusted source for stylish furniture, lighting, rugs, accessories and mattresses. For every taste and budget\n"
     ]
    }
   ],
   "source": [
    "# Locate the \"About this item\" section\n",
    "about_section = soup.find('div', id='feature-bullets')\n",
    "\n",
    "# Find all list items in this section\n",
    "list_items = about_section.find_all('span', class_='a-list-item') if about_section else []\n",
    "\n",
    "# Extract and concatenate the text of each item\n",
    "about_text = ' '.join(item.get_text(strip=True) for item in list_items)\n",
    "\n",
    "# Print the result\n",
    "print(about_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://m.media-amazon.com/images/W/MEDIAX_792452-T2/images/I/81VeuDySF-L._AC_SX355_.jpg\n"
     ]
    }
   ],
   "source": [
    "# Find the img tag\n",
    "img_tag = soup.find('img', alt=product_title)\n",
    "\n",
    "# Extract the URL from the src attribute\n",
    "img_url = img_tag['src'] if img_tag else 'Image URL not found'\n",
    "\n",
    "# Print the image URL\n",
    "print(img_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 함수화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "def fetch_info_from_product_page(product_url):\n",
    "    \n",
    "        # Set up Chrome options\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Run in headless mode (without opening a browser window)\n",
    "    chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\")\n",
    "\n",
    "    # Initialize the driver\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "    # fetch whole html from url\n",
    "    driver.get(product_url)\n",
    "    html_content = driver.page_source\n",
    "    \n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Get each information    \n",
    "    \n",
    "    # title\n",
    "    title = soup.find('span', {'id': 'productTitle'}).get_text(strip=True)\n",
    "\n",
    "    # price\n",
    "    price_whole_part = soup.find('span', class_='a-price-whole')\n",
    "    price_fraction_part = soup.find('span', class_='a-price-fraction')\n",
    "    price = f'${price_whole_part.get_text(strip=True)}{price_fraction_part.get_text(strip=True)}' if price_whole_part and price_fraction_part else 'Price not Found'\n",
    "\n",
    "    # product info (manufacturer)\n",
    "    product_info = {}\n",
    "    table = soup.find('table', class_='a-normal a-spacing-micro')\n",
    "    if table:\n",
    "        # Loop through each row in the table\n",
    "        for row in table.find_all('tr'):\n",
    "            # Extract columns: key and value\n",
    "            columns = row.find_all('td')\n",
    "            if len(columns) == 2:  # Ensure that there are exactly 2 columns\n",
    "                key = columns[0].get_text(strip=True)\n",
    "                value = columns[1].get_text(strip=True)\n",
    "                product_info[key] = value\n",
    "    else:\n",
    "        # Handle the case where the table is not found\n",
    "        product_info = None  # or {} if you prefer an empty dictionary\n",
    "\n",
    "\n",
    "    # product feature\n",
    "    tables = soup.find_all('table', class_='a-normal')\n",
    "    product_features = {}\n",
    "    for table in tables:\n",
    "        rows = table.find_all('tr')\n",
    "        for row in rows:\n",
    "            # Extract the feature name\n",
    "            feature_name_element = row.find('span', class_='a-text-bold')\n",
    "            if feature_name_element:\n",
    "                feature_name = feature_name_element.get_text(strip=True)\n",
    "                # Extract the feature value\n",
    "                feature_values = row.find_all('span', class_='a-size-base handle-overflow')\n",
    "                if feature_values:\n",
    "                    feature_value = feature_values[-1].get_text(strip=True)\n",
    "                    product_features[feature_name] = feature_value\n",
    "    \n",
    "    # product detail text\n",
    "    about_section = soup.find('div', id='feature-bullets')\n",
    "    list_items = about_section.find_all('span', class_='a-list-item') if about_section else []\n",
    "    product_text = ' '.join(item.get_text(strip=True) for item in list_items)\n",
    "\n",
    "    \n",
    "    # return\n",
    "    return title, price, product_info, product_features, product_text\n",
    " #   return string, string, dict, dict, string \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Fetch data and store it in a DataFrame\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_info_from_product_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproduct_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttps://www.amazon.com/Tornama-Contrasting-Storage-Convertible-Sectional/dp/B0CFF8BJGR/ref=sr_1_6?fst=as\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43m3Aoff&pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_r=0X2CMRE53H2HSQR09781&pf_rd_s=merchandised-search-2&pf_rd_t=101&qid=1702449247&refinements=p_n_feature_two_browse-bin\u001b[39;49m\u001b[38;5;124;43m%\u001b[39;49m\u001b[38;5;124;43m3A3248836011&rnid=3248834011&s=home-garden&sr=1-6\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([data], columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTitle\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrice\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProductInfo\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProductFeatures\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProductText\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Save the DataFrame to a CSV file\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 15\u001b[0m, in \u001b[0;36mfetch_info_from_product_page\u001b[1;34m(product_url)\u001b[0m\n\u001b[0;32m     12\u001b[0m driver \u001b[38;5;241m=\u001b[39m webdriver\u001b[38;5;241m.\u001b[39mChrome(options\u001b[38;5;241m=\u001b[39mchrome_options)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# fetch whole html from url\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproduct_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m html_content \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mpage_source\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Parse the HTML content\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\tf\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:357\u001b[0m, in \u001b[0;36mWebDriver.get\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    356\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Loads a web page in the current browser session.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 357\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGET\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43murl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\tf\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:346\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[1;34m(self, driver_command, params)\u001b[0m\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[0;32m    344\u001b[0m         params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession_id\n\u001b[1;32m--> 346\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommand_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver_command\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\tf\\Lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:300\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[1;34m(self, command, params)\u001b[0m\n\u001b[0;32m    298\u001b[0m data \u001b[38;5;241m=\u001b[39m utils\u001b[38;5;241m.\u001b[39mdump_json(params)\n\u001b[0;32m    299\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 300\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\tf\\Lib\\site-packages\\selenium\\webdriver\\remote\\remote_connection.py:321\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[1;34m(self, method, url, body)\u001b[0m\n\u001b[0;32m    318\u001b[0m     body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_alive:\n\u001b[1;32m--> 321\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    322\u001b[0m     statuscode \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus\n\u001b[0;32m    323\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\tf\\Lib\\site-packages\\urllib3\\_request_methods.py:118\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[1;34m(self, method, url, body, fields, headers, json, **urlopen_kw)\u001b[0m\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_url(\n\u001b[0;32m    111\u001b[0m         method,\n\u001b[0;32m    112\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw,\n\u001b[0;32m    116\u001b[0m     )\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 118\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_encode_body\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43murlopen_kw\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\tf\\Lib\\site-packages\\urllib3\\_request_methods.py:217\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_body\u001b[1;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[0;32m    213\u001b[0m     extra_kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m, content_type)\n\u001b[0;32m    215\u001b[0m extra_kw\u001b[38;5;241m.\u001b[39mupdate(urlopen_kw)\n\u001b[1;32m--> 217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\tf\\Lib\\site-packages\\urllib3\\poolmanager.py:444\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[1;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[0;32m    442\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    443\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 444\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    446\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\tf\\Lib\\site-packages\\urllib3\\connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[1;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[0;32m    787\u001b[0m response_conn \u001b[38;5;241m=\u001b[39m conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    789\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[1;32m--> 790\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    805\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[0;32m    806\u001b[0m clean_exit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\tf\\Lib\\site-packages\\urllib3\\connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[1;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;66;03m# Receive the response from the server\u001b[39;00m\n\u001b[0;32m    535\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 536\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (BaseSSLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_timeout(err\u001b[38;5;241m=\u001b[39me, url\u001b[38;5;241m=\u001b[39murl, timeout_value\u001b[38;5;241m=\u001b[39mread_timeout)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\tf\\Lib\\site-packages\\urllib3\\connection.py:461\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mresponse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HTTPResponse\n\u001b[0;32m    460\u001b[0m \u001b[38;5;66;03m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[1;32m--> 461\u001b[0m httplib_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    464\u001b[0m     assert_header_parsing(httplib_response\u001b[38;5;241m.\u001b[39mmsg)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\tf\\Lib\\http\\client.py:1378\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1376\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1377\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1378\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1379\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1380\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\tf\\Lib\\http\\client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 318\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    320\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\tf\\Lib\\http\\client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 279\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\tf\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# test for saving data into df, csv.\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Fetch data and store it in a DataFrame\n",
    "data = fetch_info_from_product_page(product_url='https://www.amazon.com/Tornama-Contrasting-Storage-Convertible-Sectional/dp/B0CFF8BJGR/ref=sr_1_6?fst=as%3Aoff&pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_r=0X2CMRE53H2HSQR09781&pf_rd_s=merchandised-search-2&pf_rd_t=101&qid=1702449247&refinements=p_n_feature_two_browse-bin%3A3248836011&rnid=3248834011&s=home-garden&sr=1-6')\n",
    "df = pd.DataFrame([data], columns=['Title', 'Price', 'ProductInfo', 'ProductFeatures', 'ProductText'])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('product_data.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "      <th>ProductInfo</th>\n",
       "      <th>ProductFeatures</th>\n",
       "      <th>ProductText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tornama 76'' L Shaped Contrasting Sofa Couch w...</td>\n",
       "      <td>$20999</td>\n",
       "      <td>{'Brand': 'Tornama', 'Assembly Required': 'Yes...</td>\n",
       "      <td>{'Seating Capacity': '3.00', 'Style': 'Modern'...</td>\n",
       "      <td>Comfortable Seating : The comfort of the sofa ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title   Price  \\\n",
       "0  Tornama 76'' L Shaped Contrasting Sofa Couch w...  $20999   \n",
       "\n",
       "                                         ProductInfo  \\\n",
       "0  {'Brand': 'Tornama', 'Assembly Required': 'Yes...   \n",
       "\n",
       "                                     ProductFeatures  \\\n",
       "0  {'Seating Capacity': '3.00', 'Style': 'Modern'...   \n",
       "\n",
       "                                         ProductText  \n",
       "0  Comfortable Seating : The comfort of the sofa ...  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the dictionary of URLs\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Iterate over each URL in the list\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, url \u001b[38;5;129;01min\u001b[39;00m index_url_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m---> 20\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_info_from_product_page\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproduct_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Write the data to the CSV file\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     writer\u001b[38;5;241m.\u001b[39mwriterow(data)\n",
      "Cell \u001b[1;32mIn[61], line 24\u001b[0m, in \u001b[0;36mfetch_info_from_product_page\u001b[1;34m(product_url)\u001b[0m\n\u001b[0;32m     19\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(html_content, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Get each information    \u001b[39;00m\n\u001b[0;32m     22\u001b[0m \n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# title\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m title \u001b[38;5;241m=\u001b[39m \u001b[43msoup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspan\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mid\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mproductTitle\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_text\u001b[49m(strip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# price\u001b[39;00m\n\u001b[0;32m     27\u001b[0m price_whole_part \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspan\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma-price-whole\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get_text'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define the CSV file path\n",
    "csv_file_path = 'temp.csv'\n",
    "\n",
    "# Check if the file exists\n",
    "file_exists = os.path.exists(csv_file_path)\n",
    "\n",
    "# Open the file in append mode ('a') if it exists, otherwise in write mode ('w')\n",
    "with open(csv_file_path, mode='a' if file_exists else 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    # If the file does not exist, write the headers\n",
    "    if not file_exists:\n",
    "        writer.writerow(['Title', 'Price', 'Product Info', 'Product Features', 'Product Text'])\n",
    "\n",
    "    # Iterate over each URL in the list\n",
    "    for key, url in index_url_dict.items():\n",
    "        data = fetch_info_from_product_page(product_url=url)\n",
    "\n",
    "        # Write the data to the CSV file\n",
    "        writer.writerow(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 상품 페이지 리스트 뽑는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nextpage_url(driver):\n",
    "    # get the html\n",
    "\n",
    "    time.sleep(1)\n",
    "    \n",
    "    html_content = driver.page_source\n",
    "    \n",
    "    # Initialize BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Find all <a> tags and filter with a lambda function for the one with 'Go to next page' in aria-label\n",
    "    next_page_tags = soup.find_all(lambda tag: tag.name == 'a' and 'aria-label' in tag.attrs and 'Go to next page' in tag['aria-label'])\n",
    "\n",
    "    # Extract the href attribute from the first matching tag and prepend with 'https://www.amazon.com'\n",
    "    next_page_url = 'https://www.amazon.com' + next_page_tags[0]['href'] if next_page_tags else None\n",
    "\n",
    "    return next_page_url\n",
    "\n",
    "\n",
    "def get_productpage_url(driver):\n",
    "    # get the html\n",
    "    time.sleep(1)\n",
    "    \n",
    "    html_content = driver.page_source\n",
    "    \n",
    "    # Initialize BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Find all divs with the 'data-index' attribute\n",
    "    divs_with_data_index = soup.find_all('div', attrs={'data-index': True})\n",
    "\n",
    "\n",
    "    urls = []\n",
    "    # Loop through the found divs and extract the href attributes\n",
    "    for div in divs_with_data_index:\n",
    "        data_index = div['data-index']\n",
    "        a_tag = div.find('a', class_='a-link-normal')\n",
    "        if a_tag and 'href' in a_tag.attrs:\n",
    "            full_url = 'https://www.amazon.com' + a_tag['href']\n",
    "            if 'help/customer' in full_url:\n",
    "                continue\n",
    "            urls.append(full_url)\n",
    "\n",
    "    return urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Sectional_Sofas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Initialize the driver\u001b[39;00m\n\u001b[0;32m      7\u001b[0m driver \u001b[38;5;241m=\u001b[39m webdriver\u001b[38;5;241m.\u001b[39mChrome(options\u001b[38;5;241m=\u001b[39mchrome_options)\n\u001b[1;32m----> 9\u001b[0m page_url \u001b[38;5;241m=\u001b[39m \u001b[43mSectional_Sofas\u001b[49m\n\u001b[0;32m     10\u001b[0m csv_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSectional_Sofas_page_urls.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Check if CSV file exists and write headers if it doesn't\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Sectional_Sofas' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Set up Chrome options\n",
    "chrome_options = Options()\n",
    "# chrome_options.add_argument(\"--headless\")  # Run in headless mode (without opening a browser window)  \n",
    "chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\")\n",
    "\n",
    "# Initialize the driver\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "page_url = Sectional_Sofas\n",
    "csv_file_path = 'Sectional_Sofas_page_urls.csv'\n",
    "\n",
    "# Check if CSV file exists and write headers if it doesn't\n",
    "headers_needed = not os.path.exists(csv_file_path)\n",
    "\n",
    "with open(csv_file_path, mode='a', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    if headers_needed:\n",
    "        writer.writerow(['Page URL'])\n",
    "        writer.writerow([page_url])\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            driver.get(page_url)\n",
    "            \n",
    "            next_page_url = get_nextpage_url(driver)\n",
    "            print(next_page_url)\n",
    "            \n",
    "            before_page_url = page_url\n",
    "            page_url = next_page_url\n",
    "            \n",
    "            if next_page_url:\n",
    "                writer.writerow([next_page_url])\n",
    "            else:\n",
    "                print('Next page URL not found, retrying the current page...')\n",
    "                page_url = before_page_url  # Retry the current page\n",
    "\n",
    "            time.sleep(random.randint(1000,10000)/1000)\n",
    "\n",
    "        except TimeoutError:\n",
    "            print(\"Page load timed out. Retrying...\")\n",
    "            time.sleep(1)\n",
    "            continue  # Retry the current page\n",
    "        except TimeoutException:\n",
    "            print(\"Page load timed out. Retrying...\")\n",
    "            time.sleep(1)\n",
    "            continue\n",
    "        except AttributeError:\n",
    "            print(\"Element not found on the page.\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            break\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sofas\n",
    "Sofa_and_Couches = {\n",
    "    #'Sectional_Sofas' : 'https://www.amazon.com/s?bbn=3733551&rh=n%3A1055398%2Cn%3A%211063498%2Cn%3A1063306%2Cn%3A1063318%2Cn%3A3733551%2Cp_n_feature_two_browse-bin%3A3248836011&dc&fst=as%3Aoff&pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_r=0X2CMRE53H2HSQR09781&pf_rd_s=merchandised-search-2&pf_rd_t=101&qid=1528841766&rnid=3248834011&ref=s9_acss_bw_cg_HarSofa_2a1_w',\n",
    "    # 'Sleeper_Sofas' : 'https://www.amazon.com/s?i=garden&bbn=3733551&rh=n%3A1055398%2Cn%3A%211063498%2Cn%3A1063306%2Cn%3A1063318%2Cn%3A3733551%2Cp_n_feature_two_browse-bin%3A3248838011&pf_rd_i=3733551&pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=3009ccc5-2584-454e-b732-ae8e525543fe&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_r=0X2CMRE53H2HSQR09781&pf_rd_r=79FTK5EJP86JTXQETKYX&pf_rd_s=merchandised-search-2&pf_rd_s=merchandised-search-2&pf_rd_t=101&pf_rd_t=101&ref=s9_acss_bw_cg_HarSofa_2b1_w',\n",
    "    'Reclining_Sofas' : 'https://www.amazon.com/s?i=garden&bbn=3733551&rh=n%3A1055398%2Cn%3A%211063498%2Cn%3A1063306%2Cn%3A1063318%2Cn%3A3733551%2Cp_n_feature_two_browse-bin%3A12012870011&pf_rd_i=3733551&pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=3009ccc5-2584-454e-b732-ae8e525543fe&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_r=0X2CMRE53H2HSQR09781&pf_rd_r=79FTK5EJP86JTXQETKYX&pf_rd_s=merchandised-search-2&pf_rd_s=merchandised-search-2&pf_rd_t=101&pf_rd_t=101&ref=s9_acss_bw_cg_HarSofa_2c1_w',\n",
    "    'LoveSeats' : 'https://www.amazon.com/s?i=garden&bbn=3733551&rh=n%3A1055398%2Cn%3A%211063498%2Cn%3A1063306%2Cn%3A1063318%2Cn%3A3733551%2Cp_n_feature_two_browse-bin%3A3248835011&pf_rd_i=3733551&pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=3009ccc5-2584-454e-b732-ae8e525543fe&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_r=79FTK5EJP86JTXQETKYX&pf_rd_r=QZ6K0G39A8CCYDMBWJRD&pf_rd_s=merchandised-search-2&pf_rd_s=merchandised-search-2&pf_rd_t=101&pf_rd_t=101&ref=s9_acss_bw_cg_HarSofa_2d1_w',\n",
    "    'Futons' : 'https://www.amazon.com/Futons/b/ref=s9_acss_bw_cg_SofaType_1f1_w/ref=s9_acss_bw_cg_HarSofa_3a1_w?ie=UTF8&node=13753041&pf_rd_m=ATVPDKIKX0DER&pf_rd_s=merchandised-search-2&pf_rd_r=79FTK5EJP86JTXQETKYX&pf_rd_t=101&pf_rd_p=3009ccc5-2584-454e-b732-ae8e525543fe&pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_s=merchandised-search-2&pf_rd_r=QZ6K0G39A8CCYDMBWJRD&pf_rd_t=101&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_i=3733551',\n",
    "    'Settles' : 'https://www.amazon.com/s?i=garden&bbn=3733551&rh=n%3A1055398%2Cn%3A%211063498%2Cn%3A1063306%2Cn%3A1063318%2Cn%3A3733551%2Cp_n_feature_two_browse-bin%3A3248837011&pf_rd_i=3733551&pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=3009ccc5-2584-454e-b732-ae8e525543fe&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_r=79FTK5EJP86JTXQETKYX&pf_rd_r=QZ6K0G39A8CCYDMBWJRD&pf_rd_s=merchandised-search-2&pf_rd_s=merchandised-search-2&pf_rd_t=101&pf_rd_t=101&ref=s9_acss_bw_cg_HarSofa_3b1_w',\n",
    "    'Convertibles' : 'https://www.amazon.com/s?bbn=3733551&rh=n%3A1055398%2Cn%3A%211063498%2Cn%3A1063306%2Cn%3A1063318%2Cn%3A3733551%2Cp_n_feature_two_browse-bin%3A12012869011&dc&fst=as%3Aoff&pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_r=QZ6K0G39A8CCYDMBWJRD&pf_rd_s=merchandised-search-2&pf_rd_t=101&qid=1528765569&rnid=3248834011&ref=s9_acss_bw_cg_HarSofa_3c1_w',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run in headless mode (without opening a browser window)  \n",
    "chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\")\n",
    "\n",
    "# Initialize the driver\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "page_url = Sofa_and_Couches['Sleeper_Sofas']\n",
    "\n",
    "driver.get(page_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.amazon.com/s?i=garden&bbn=3733551&rh=n%3A1055398%2Cn%3A1063498%2Cn%3A1063306%2Cn%3A1063318%2Cn%3A3733551%2Cp_n_feature_two_browse-bin%3A12012870011&page=2&pf_rd_i=3733551&pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=3009ccc5-2584-454e-b732-ae8e525543fe&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_r=0X2CMRE53H2HSQR09781&pf_rd_r=79FTK5EJP86JTXQETKYX&pf_rd_s=merchandised-search-2&pf_rd_s=merchandised-search-2&pf_rd_t=101&pf_rd_t=101&qid=1702470064&ref=sr_pg_1\n",
      "None\n",
      "Next page URL not found, retrying the current page...\n",
      "None\n",
      "Next page URL not found, retrying the current page...\n",
      "None\n",
      "Next page URL not found, retrying the current page...\n",
      "None\n",
      "Next page URL not found, retrying the current page...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. 셀의 코드를 검토하여 오류의 가능한 원인을 식별하세요. 자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'> 여기 </a> 를 클릭하세요. 자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "for name, url in Sofa_and_Couches.items():\n",
    "    \n",
    "    # Set up Chrome options\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")  # Run in headless mode (without opening a browser window)  \n",
    "    chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\")\n",
    "\n",
    "    # Initialize the driver\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "    page_url = url\n",
    "    csv_file_path = f'{name}_page_urls.csv'\n",
    "\n",
    "    # Check if CSV file exists and write headers if it doesn't\n",
    "    headers_needed = not os.path.exists(csv_file_path)\n",
    "\n",
    "    with open(csv_file_path, mode='a', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "\n",
    "        if headers_needed:\n",
    "            writer.writerow(['Page URL'])\n",
    "            writer.writerow([page_url])\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                driver.get(page_url)\n",
    "                time.sleep(1)\n",
    "                \n",
    "                next_page_url = get_nextpage_url(driver)\n",
    "                print(next_page_url)\n",
    "                \n",
    "                before_page_url = page_url\n",
    "                page_url = next_page_url\n",
    "                \n",
    "                if next_page_url:\n",
    "                    writer.writerow([next_page_url])\n",
    "                else:\n",
    "                    print('Next page URL not found, retrying the current page...')\n",
    "                    page_url = before_page_url  # Retry the current page\n",
    "\n",
    "                time.sleep(random.randint(1000,10000)/1000)\n",
    "\n",
    "            except TimeoutError:\n",
    "                print(\"Page load timed out. Retrying...\")\n",
    "                time.sleep(1)\n",
    "                continue  # Retry the current page\n",
    "            except TimeoutException:\n",
    "                print(\"Page load timed out. Retrying...\")\n",
    "                time.sleep(1)\n",
    "                continue\n",
    "            except AttributeError:\n",
    "                print(\"Element not found on the page.\")\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                break\n",
    "\n",
    "    # Close the driver\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-------------------------------------------------------------------------------'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "from email import header\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException\n",
    "\n",
    "from selenium_stealth import stealth\n",
    "\n",
    "from browsermobproxy import Server\n",
    "from amazoncaptcha import AmazonCaptcha\n",
    "\n",
    "\n",
    "import time, os, csv, random, html, re\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "'-------------------------------------------------------------------------------'\n",
    "\n",
    "# Function to click on next page link\n",
    "def click_next_page(driver):\n",
    "    try:\n",
    "        # Find the 'Next Page' link using its aria-label\n",
    "        next_page_link = driver.find_element(By.XPATH, \"//a[contains(@aria-label, 'Go to next page')]\")\n",
    "\n",
    "        # If the link is found, click it\n",
    "        if next_page_link:\n",
    "            actions = ActionChains(driver)\n",
    "            actions.move_to_element(next_page_link).click().perform()\n",
    "\n",
    "            # Optionally, wait for the page to load after clicking\n",
    "            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
    "            time.sleep(random.randint(2000, 5000)/1000)  # Random sleep to mimic human behavior\n",
    "\n",
    "            # Return the driver object\n",
    "            return True, driver\n",
    "        \n",
    "    except NoSuchElementException:\n",
    "        # If the link is not found\n",
    "        print(\"Next page link not found.\")\n",
    "        return False, driver  # Return None to indicate that the next page link was not found\n",
    "\n",
    "    \n",
    "# Get Next Page's URL\n",
    "def get_nextpage_url(driver):\n",
    "    try:\n",
    "        # get the html\n",
    "\n",
    "        time.sleep(1)\n",
    "        \n",
    "        html_content = driver.page_source\n",
    "        \n",
    "        # Initialize BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Find all <a> tags and filter with a lambda function for the one with 'Go to next page' in aria-label\n",
    "        next_page_tags = soup.find_all(lambda tag: tag.name == 'a' and 'aria-label' in tag.attrs and 'Go to next page' in tag['aria-label'])\n",
    "\n",
    "        # Extract the href attribute from the first matching tag and prepend with 'https://www.amazon.com'\n",
    "        next_page_url = 'https://www.amazon.com' + next_page_tags[0]['href'] if next_page_tags else None\n",
    "\n",
    "        return next_page_url\n",
    "    except Exception as e:\n",
    "        print('get_nextpage_url error: ', e)\n",
    "        return None\n",
    "        \n",
    "'-------------------------------------------------------------------------------'\n",
    "\n",
    "# Get Each Product's URL from page\n",
    "def get_productpage_url(driver):\n",
    "    try:\n",
    "        # get the html\n",
    "        time.sleep(1)\n",
    "        \n",
    "        html_content = driver.page_source\n",
    "        \n",
    "        # Initialize BeautifulSoup\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        # Find all divs with the 'data-index' attribute\n",
    "        divs_with_data_index = soup.find_all('div', attrs={'data-index': True})\n",
    "\n",
    "\n",
    "        urls = []\n",
    "        # Loop through the found divs and extract the href attributes\n",
    "        for div in divs_with_data_index:\n",
    "            data_index = div['data-index']\n",
    "            a_tag = div.find('a', class_='a-link-normal')\n",
    "            if a_tag and 'href' in a_tag.attrs:\n",
    "                full_url = 'https://www.amazon.com' + a_tag['href']\n",
    "                if 'help/customer' in full_url:\n",
    "                    continue\n",
    "                if 'aax-us-iad.amazon.com' in full_url:\n",
    "                    continue\n",
    "                \n",
    "                if type(full_url) == str:\n",
    "                    urls.append([full_url])\n",
    "\n",
    "        return urls\n",
    "    \n",
    "    except Exception as e:\n",
    "        print('Get ProductPage Error: ', e)\n",
    "        return []\n",
    "    \n",
    "\n",
    "'-------------------------------------------------------------------------------'\n",
    "\n",
    "# Fetch Data from product page\n",
    "def fetch_info_from_product_page(driver):\n",
    "    try:\n",
    "        # fetch whole html from url\n",
    "        html_content = driver.page_source\n",
    "        \n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "        output = []\n",
    "        # Get each information    \n",
    "        \n",
    "        # category\n",
    "        breadcrumb_links = soup.select('#wayfinding-breadcrumbs_feature_div ul a')\n",
    "        category = [html.unescape(link.get_text().strip()) for link in breadcrumb_links]\n",
    "        output.append(category)\n",
    "        \n",
    "        # title\n",
    "        title = soup.find('span', {'id': 'productTitle'}).get_text(strip=True)\n",
    "        output.append(title)\n",
    "\n",
    "        # price\n",
    "        price_whole_part = soup.find('span', class_='a-price-whole')\n",
    "        price_fraction_part = soup.find('span', class_='a-price-fraction')\n",
    "        price = f'${price_whole_part.get_text(strip=True)}{price_fraction_part.get_text(strip=True)}' if price_whole_part and price_fraction_part else 'Price not Found'\n",
    "        output.append(price)\n",
    "        \n",
    "        # product info (manufacturer)\n",
    "        product_info = {}\n",
    "        table = soup.find('table', class_='a-normal a-spacing-micro')\n",
    "        if table:\n",
    "            # Loop through each row in the table\n",
    "            for row in table.find_all('tr'):\n",
    "                # Extract columns: key and value\n",
    "                columns = row.find_all('td')\n",
    "                if len(columns) == 2:  # Ensure that there are exactly 2 columns\n",
    "                    key = columns[0].get_text(strip=True)\n",
    "                    value = columns[1].get_text(strip=True)\n",
    "                    product_info[key] = value\n",
    "        else:\n",
    "            # Handle the case where the table is not found\n",
    "            product_info = {}  # or None if you prefer None\n",
    "        output.append(product_info)\n",
    "\n",
    "        # product feature\n",
    "        tables = soup.find_all('table', class_='a-normal')\n",
    "        product_features = {}\n",
    "        for table in tables:\n",
    "            rows = table.find_all('tr')\n",
    "            for row in rows:\n",
    "                # Extract the feature name\n",
    "                feature_name_element = row.find('span', class_='a-text-bold')\n",
    "                if feature_name_element:\n",
    "                    feature_name = feature_name_element.get_text(strip=True)\n",
    "                    # Extract the feature value\n",
    "                    feature_values = row.find_all('span', class_='a-size-base handle-overflow')\n",
    "                    if feature_values:\n",
    "                        feature_value = feature_values[-1].get_text(strip=True)\n",
    "                        product_features[feature_name] = feature_value\n",
    "        output.append(product_features)\n",
    "        \n",
    "        # product detail text\n",
    "        about_section = soup.find('div', id='feature-bullets')\n",
    "        list_items = about_section.find_all('span', class_='a-list-item') if about_section else []\n",
    "        product_text = ' '.join(item.get_text(strip=True) for item in list_items)\n",
    "        output.append(product_text)\n",
    "        \n",
    "        # img_url\n",
    "        selected_imgs = soup.select('#imgTagWrapperId img, .imgTagWrapper img')\n",
    "        for img in selected_imgs:\n",
    "            image_url = img.get('data-old-hires', img.get('src'))  # Default to src if data-old-hires is not present\n",
    "        output.append(image_url)\n",
    "        \n",
    "        # return\n",
    "        return output #  = [category, title, price, product_info, product_features, product_text, image_url]\n",
    "\n",
    "    except Exception as e:\n",
    "        print('Fetch Info Error: ', e)        \n",
    "        return []\n",
    "\n",
    "def sanitize_filename(filename):\n",
    "    # Keep only alphabets (uppercase and lowercase)\n",
    "    return re.sub(r'[^a-zA-Z]', '', filename)\n",
    "\n",
    "def save_img(name, title, image_url):\n",
    "    try:\n",
    "        # Send a GET request to the image URL\n",
    "        response = requests.get(image_url)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "\n",
    "        # Create directory if it doesn't exist\n",
    "        os.makedirs(f'imgs/{name}', exist_ok=True)\n",
    "        \n",
    "        # Sanitize title for filename\n",
    "        sanitized_title = sanitize_filename(title[:200])\n",
    "        file_path = f'imgs/{name}/{sanitized_title}.jpg'\n",
    "\n",
    "        # Check if the image is already in JPEG format\n",
    "        if image_url.lower().endswith('.jpg'):\n",
    "            # Save image directly if it is already a JPEG\n",
    "            with open(file_path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "            return True  # return True when Success\n",
    "        else:\n",
    "            # Convert and save in JPEG format if the image is in a different format\n",
    "            img = Image.open(BytesIO(response.content))\n",
    "            img.convert('RGB').save(file_path, 'JPEG')\n",
    "            return True  # return True when Success\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading image: {e}\")\n",
    "        return False  # return False when failure\n",
    "    \n",
    "'-------------------------------------------------------------------------------'\n",
    "\n",
    "# configure selenium chrome driver\n",
    "def configure_driver():\n",
    "    try:\n",
    "        # # Start the BrowserMob Proxy server\n",
    "        # server = Server(r\"C:\\browsermob-proxy-2.1.4\\bin\\browsermob-proxy\")\n",
    "        # server.start()\n",
    "        # proxy = server.create_proxy()\n",
    "            \n",
    "        # chromedriver_path = '/home/bae/.cache/selenium/chromedriver/linux64/120.0.6099.71/chromedriver'\n",
    "        # possible paths\n",
    "        # /home/bae/.cache/selenium/chromedriver\n",
    "        # /home/bae/.cache/selenium/chromedriver/linux64/120.0.6099.71/chromedriver\n",
    "        # /mnt/c/Users/user/.cache/selenium/chromedriver\n",
    "        # service = Service(chromedriver_path)\n",
    "        \n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 11.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.6099.110 Safari/537.36\")\n",
    "        # chrome_options.add_argument(\"--proxy-server={0}\".format(proxy.proxy)) # Set up Selenium to use the proxy\n",
    "        chrome_options.add_argument(\"--disable-gpu\")\n",
    "        # chrome_options.add_argument(\"--headless\")\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        chrome_options.add_argument(\"--disable-software-rasterizer\")    \n",
    "        chrome_options.add_argument(\"disable-infobars\")\n",
    "        chrome_options.add_argument(\"--disable-extensions\")\n",
    "        # chrome_options.add_argument(\"--remote-debugging-port=9222\")\n",
    "        \n",
    "        driver = webdriver.Chrome(options=chrome_options)\n",
    "        \n",
    "        # Create custom headers (including Referer)\n",
    "        headers = {\n",
    "            \"Referer\": \"https://www.amazon.com/Living-Room-Furniture/b?node=3733551\",\n",
    "            # Add any other custom headers you want\n",
    "        }\n",
    "        # proxy.add_to_capabilities(headers)\n",
    "        \n",
    "        # Apply stealth settings    \n",
    "        stealth(driver,\n",
    "                languages=[\"en-US\", \"en\"],  # List of languages\n",
    "                vendor=\"Google Inc.\",  # Vendor name\n",
    "                platform=\"Win64\",  # Platform\n",
    "                webgl_vendor=\"Intel Inc.\",  # WebGL vendor\n",
    "                renderer=\"Intel Iris OpenGL Engine\",  # WebGL renderer\n",
    "                fix_hairline=True,  # Fix for thin lines issue\n",
    "                )\n",
    "        \n",
    "        return True, driver\n",
    "    \n",
    "    except Exception as e:\n",
    "        print('Error Configuring Driver: ',e)\n",
    "        return False, driver\n",
    "\n",
    "def safe_get(driver, url):\n",
    "    max_retries = 5\n",
    "    attempts = 0\n",
    "    while attempts < max_retries:\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))\n",
    "            # driver.execute_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "            time.sleep(random.randint(2000, 5000)/1000)\n",
    "            \n",
    "            # Find the captcha\n",
    "            html_content = driver.page_source\n",
    "            soup = BeautifulSoup(html_content, 'html.parser')\n",
    "            captcha_text = soup.find('h4', text=\"Type the characters you see in this image:\")\n",
    "            if captcha_text:\n",
    "                captcha_image = soup.find('img')\n",
    "                captcha_image_url = captcha_image['src'] if captcha_image else None\n",
    "                if captcha_image_url:\n",
    "                    captcha = AmazonCaptcha.fromlink(captcha_image_url)\n",
    "                    solution = captcha.solve()\n",
    "                    \n",
    "            return True, driver  # Page load successful\n",
    "        \n",
    "        except (TimeoutException, WebDriverException) as e:\n",
    "            print(f\"Network error encountered: {e}. Retrying...\")\n",
    "            time.sleep(random.randint(2000, 5000)/1000)\n",
    "            attempts += 1\n",
    "       \n",
    "        except Exception as e:\n",
    "            print('Error in safe_get: ', e)\n",
    "            time.sleep(random.randint(2000, 5000)/1000)\n",
    "            attempts += 1\n",
    "\n",
    "    return False, driver  # Page load failed\n",
    "\n",
    "'-------------------------------------------------------------------------------'\n",
    "\n",
    "Sofa_and_Couches = {\n",
    "    'Sectional_Sofas' : 'https://www.amazon.com/s?bbn=3733551&rh=n%3A1055398%2Cn%3A%211063498%2Cn%3A1063306%2Cn%3A1063318%2Cn%3A3733551%2Cp_n_feature_two_browse-bin%3A3248836011&dc&fst=as%3Aoff&pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_r=0X2CMRE53H2HSQR09781&pf_rd_s=merchandised-search-2&pf_rd_t=101&qid=1528841766&rnid=3248834011&ref=s9_acss_bw_cg_HarSofa_2a1_w',\n",
    "#     'Sleeper_Sofas' : 'https://www.amazon.com/s?i=garden&bbn=3733551&rh=n%3A1055398%2Cn%3A%211063498%2Cn%3A1063306%2Cn%3A1063318%2Cn%3A3733551%2Cp_n_feature_two_browse-bin%3A3248838011&pf_rd_i=3733551&pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=3009ccc5-2584-454e-b732-ae8e525543fe&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_r=0X2CMRE53H2HSQR09781&pf_rd_r=79FTK5EJP86JTXQETKYX&pf_rd_s=merchandised-search-2&pf_rd_s=merchandised-search-2&pf_rd_t=101&pf_rd_t=101&ref=s9_acss_bw_cg_HarSofa_2b1_w',\n",
    "#     'Reclining_Sofas' : 'https://www.amazon.com/s?i=garden&bbn=3733551&rh=n%3A1055398%2Cn%3A%211063498%2Cn%3A1063306%2Cn%3A1063318%2Cn%3A3733551%2Cp_n_feature_two_browse-bin%3A12012870011&pf_rd_i=3733551&pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=3009ccc5-2584-454e-b732-ae8e525543fe&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_r=0X2CMRE53H2HSQR09781&pf_rd_r=79FTK5EJP86JTXQETKYX&pf_rd_s=merchandised-search-2&pf_rd_s=merchandised-search-2&pf_rd_t=101&pf_rd_t=101&ref=s9_acss_bw_cg_HarSofa_2c1_w',\n",
    "#     'LoveSeats' : 'https://www.amazon.com/s?i=garden&bbn=3733551&rh=n%3A1055398%2Cn%3A%211063498%2Cn%3A1063306%2Cn%3A1063318%2Cn%3A3733551%2Cp_n_feature_two_browse-bin%3A3248835011&pf_rd_i=3733551&pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=3009ccc5-2584-454e-b732-ae8e525543fe&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_r=79FTK5EJP86JTXQETKYX&pf_rd_r=QZ6K0G39A8CCYDMBWJRD&pf_rd_s=merchandised-search-2&pf_rd_s=merchandised-search-2&pf_rd_t=101&pf_rd_t=101&ref=s9_acss_bw_cg_HarSofa_2d1_w',\n",
    "#     'Futons' : 'https://www.amazon.com/Futons/b/ref=s9_acss_bw_cg_SofaType_1f1_w/ref=s9_acss_bw_cg_HarSofa_3a1_w?ie=UTF8&node=13753041&pf_rd_m=ATVPDKIKX0DER&pf_rd_s=merchandised-search-2&pf_rd_r=79FTK5EJP86JTXQETKYX&pf_rd_t=101&pf_rd_p=3009ccc5-2584-454e-b732-ae8e525543fe&pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_s=merchandised-search-2&pf_rd_r=QZ6K0G39A8CCYDMBWJRD&pf_rd_t=101&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_i=3733551',\n",
    "#     'Settles' : 'https://www.amazon.com/s?i=garden&bbn=3733551&rh=n%3A1055398%2Cn%3A%211063498%2Cn%3A1063306%2Cn%3A1063318%2Cn%3A3733551%2Cp_n_feature_two_browse-bin%3A3248837011&pf_rd_i=3733551&pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=3009ccc5-2584-454e-b732-ae8e525543fe&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_r=79FTK5EJP86JTXQETKYX&pf_rd_r=QZ6K0G39A8CCYDMBWJRD&pf_rd_s=merchandised-search-2&pf_rd_s=merchandised-search-2&pf_rd_t=101&pf_rd_t=101&ref=s9_acss_bw_cg_HarSofa_3b1_w',\n",
    "#     'Convertibles' : 'https://www.amazon.com/s?bbn=3733551&rh=n%3A1055398%2Cn%3A%211063498%2Cn%3A1063306%2Cn%3A1063318%2Cn%3A3733551%2Cp_n_feature_two_browse-bin%3A12012869011&dc&fst=as%3Aoff&pf_rd_i=3733551&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_r=QZ6K0G39A8CCYDMBWJRD&pf_rd_s=merchandised-search-2&pf_rd_t=101&qid=1528765569&rnid=3248834011&ref=s9_acss_bw_cg_HarSofa_3c1_w',\n",
    "}\n",
    "\n",
    "# Other_Livingroom_Furniture = {\n",
    "#     'Accent_Chairs' : 'https://www.amazon.com/b?node=3733491&ref=s9_acss_bw_cg_SBR2019_3b1_w', # 42 pages\n",
    "#     'Coffee_Tables' : 'https://www.amazon.com/b?node=3733631&ref=s9_acss_bw_cg_SBR2019_3c1_w', # 35 pages\n",
    "#     'TV_Stands' : 'https://www.amazon.com/b?node=14109851&ref=s9_acss_bw_cg_SBR2019_3d1_w',\n",
    "#     'End_Tables' : 'https://www.amazon.com/b?node=3733641&ref=s9_acss_bw_cg_SBR2019_4a1_w',\n",
    "#     'Console_Tables' : 'https://www.amazon.com/b?node=3733651&ref=s9_acss_bw_cg_SBR2019_4b1_w',\n",
    "#     'Ottomans' : 'https://amazon.com/b/ref=sv_hg_fl_3254639011/ref=s9_acss_bw_cg_SBR2019_4c1_w?ie=UTF8&node=3254639011&pf_rd_m=ATVPDKIKX0DER&pf_rd_s=merchandised-search-3&pf_rd_r=HH88E0EETR0MQZ658N3D&pf_rd_t=101&pf_rd_p=f126b4c9-d5f7-41d5-85e0-9db2657ec29e&pf_rd_i=14544497011',\n",
    "#     'Living_Room_Sets' : 'https://www.amazon.com/b?node=3733481&ref=s9_acss_bw_cg_SBR2019_4d1_w',\n",
    "# }\n",
    "\n",
    "# Decor_and_Soft_Furnishings = {\n",
    "#     'Decorative_Pillows' : 'https://www.amazon.com/s?rh=n%3A3732321&fs=true&ref=lp_3732321_sar', # 161 pages\n",
    "#     'Throw_Blankets' : 'https://www.amazon.com/s?rh=n%3A14058581&fs=true&ref=lp_14058581_sar', # 400 pages\n",
    "#     'Area_Rugs' : 'https://www.amazon.com/s?rh=n%3A684541011&fs=true&ref=lp_684541011_sar', # 400 pages\n",
    "#     'Wall_Arts' : 'https://www.amazon.com/s?rh=n%3A3736081&fs=true&ref=lp_3736081_sar' , # 400 pages\n",
    "#     'Table_Lamps' : 'https://www.amazon.com/b?node=1063296&ref=s9_acss_bw_cg_SBR2019_7a1_w', # 347 pages\n",
    "#     'Floor_Lamps' : 'https://www.amazon.com/b?node=1063294&ref=s9_acss_bw_cg_SBR2019_7b1_w', # 131 pages\n",
    "#     'Pendants_and_Chandeliers' : 'https://www.amazon.com/lighting-ceiling-fans/b/ref=s9_acss_bw_cg_SBR2019_7c1_w?ie=UTF8&node=495224&ref_=sv_hg_5&pf_rd_m=ATVPDKIKX0DER&pf_rd_s=merchandised-search-3&pf_rd_r=HH88E0EETR0MQZ658N3D&pf_rd_t=101&pf_rd_p=f126b4c9-d5f7-41d5-85e0-9db2657ec29e&pf_rd_i=14544497011', #298 pages\n",
    "#     'Sconces' : 'https://www.amazon.com/b?node=3736721&ref=s9_acss_bw_cg_SBR2019_7d1_w', # 238 pages\n",
    "#     'Baskets_and_Storage' : 'https://www.amazon.com/s?rh=n%3A2422430011&fs=true&ref=lp_2422430011_sar', # 244 pages\n",
    "#     'Candles' : 'https://www.amazon.com/s?rh=n%3A3734391&fs=true&ref=lp_3734391_sar', # 400 pages\n",
    "#     'Live_Plants' : 'https://www.amazon.com/b/ref=sv_hg_fl_553798/ref=s9_acss_bw_cg_SBR2019_8c1_w?node=3480662011&pf_rd_m=ATVPDKIKX0DER&pf_rd_s=merchandised-search-3&pf_rd_r=HH88E0EETR0MQZ658N3D&pf_rd_t=101&pf_rd_p=f126b4c9-d5f7-41d5-85e0-9db2657ec29e&pf_rd_i=14544497011', # 14 pages\n",
    "#     'Artificial_Plants' : 'https://www.amazon.com/b?node=14087351&ref=s9_acss_bw_cg_SBR2019_8d1_w', # 400 pags    \n",
    "#     'Planters' : 'https://www.amazon.com/b?node=553798&ref=s9_acss_bw_cg_SBR2019_9a1_w', # 263 pages\n",
    "#     'Decorative_Accessories' : 'https://www.amazon.com/s?rh=n%3A3295676011&fs=true&ref=lp_3295676011_sar', # 400 pages\n",
    "#     'Window_Coverings' : 'https://www.amazon.com/b?node=1063302&ref=s9_acss_bw_cg_SBR2019_9c1_w', # 400 pages\n",
    "#     'Decorative_Mirrors' : 'https://www.amazon.com/b?node=3736371&ref=s9_acss_bw_cg_SBR2019_9d1_w', # 314 pages\n",
    "# }\n",
    "\n",
    "# Kitchen_and_Dining_Furnitures = {\n",
    "#     'Dining_Sets' : 'https://www.amazon.com/b/ref=s9_acss_bw_cg_HarFK_2a1_w?node=8566630011&pf_rd_m=ATVPDKIKX0DER&pf_rd_s=merchandised-search-2&pf_rd_r=RDTFVCNEA5ZFMYQZYZ3E&pf_rd_t=101&pf_rd_p=fde2e008-b6c2-4e44-9488-63f47804b655&pf_rd_i=3733781',\n",
    "#     'Dining_Tables' : 'https://www.amazon.com/b/ref=s9_acss_bw_cg_HarFK_2b1_w?node=3733811&pf_rd_m=ATVPDKIKX0DER&pf_rd_s=merchandised-search-2&pf_rd_r=RDTFVCNEA5ZFMYQZYZ3E&pf_rd_t=101&pf_rd_p=fde2e008-b6c2-4e44-9488-63f47804b655&pf_rd_i=3733781',\n",
    "#     'Dining_Chairs' : 'https://www.amazon.com/b/ref=s9_acss_bw_cg_HarFK_2c1_w?node=3733821&pf_rd_m=ATVPDKIKX0DER&pf_rd_s=merchandised-search-2&pf_rd_r=RDTFVCNEA5ZFMYQZYZ3E&pf_rd_t=101&pf_rd_p=fde2e008-b6c2-4e44-9488-63f47804b655&pf_rd_i=3733781',\n",
    "#     'Bar_Stools' : 'https://www.amazon.com/b/ref=s9_acss_bw_cg_HarFK_2d1_w?node=3733851&pf_rd_m=ATVPDKIKX0DER&pf_rd_s=merchandised-search-2&pf_rd_r=RDTFVCNEA5ZFMYQZYZ3E&pf_rd_t=101&pf_rd_p=fde2e008-b6c2-4e44-9488-63f47804b655&pf_rd_i=3733781',\n",
    "#     'Kitchen_Islands' : 'https://www.amazon.com/b/ref=s9_acss_bw_cg_HarFK_3a1_w?node=8521400011&pf_rd_m=ATVPDKIKX0DER&pf_rd_s=merchandised-search-2&pf_rd_r=RDTFVCNEA5ZFMYQZYZ3E&pf_rd_t=101&pf_rd_p=fde2e008-b6c2-4e44-9488-63f47804b655&pf_rd_i=3733781',\n",
    "#     'Buffets_and_Sideboards' : 'https://www.amazon.com/b/ref=s9_acss_bw_cg_HarFK_3b1_w?node=3733831&pf_rd_m=ATVPDKIKX0DER&pf_rd_s=merchandised-search-2&pf_rd_r=RDTFVCNEA5ZFMYQZYZ3E&pf_rd_t=101&pf_rd_p=fde2e008-b6c2-4e44-9488-63f47804b655&pf_rd_i=3733781',\n",
    "#     'China_Cabinets' : 'https://www.amazon.com/b/ref=s9_acss_bw_cg_HarFK_3c1_w?node=3733841&pf_rd_m=ATVPDKIKX0DER&pf_rd_s=merchandised-search-2&pf_rd_r=RDTFVCNEA5ZFMYQZYZ3E&pf_rd_t=101&pf_rd_p=fde2e008-b6c2-4e44-9488-63f47804b655&pf_rd_i=3733781',\n",
    "#     'Bakers_Recks' : 'https://www.amazon.com/s?rh=n%3A3744061&fs=true&ref=lp_3744061_sar',\n",
    "# }\n",
    "\n",
    "# Bedroom_Furnitures = {\n",
    "#     'Bedroom_Sets' : 'https://www.amazon.com/b/ref=s9_acss_bw_cg_HarFLiv_2a1_w?node=3732931&pf_rd_m=ATVPDKIKX0DER&pf_rd_s=merchandised-search-2&pf_rd_r=MNGW2YC86GM462Z294K6&pf_rd_t=101&pf_rd_p=d8003c9c-74d3-490c-ae40-6aac6ed45f61&pf_rd_i=1063308',\n",
    "#     'Mattresses' : 'https://www.amazon.com/s?rh=n%3A3732961&fs=true&ref=lp_3732961_sar',\n",
    "#     'Nightstands' : 'https://www.amazon.com/b/ref=s9_acss_bw_cg_HarFLiv_2c1_w?node=3733251&pf_rd_m=ATVPDKIKX0DER&pf_rd_s=merchandised-search-2&pf_rd_r=MNGW2YC86GM462Z294K6&pf_rd_t=101&pf_rd_p=d8003c9c-74d3-490c-ae40-6aac6ed45f61&pf_rd_i=1063308',\n",
    "#     'Dressers' : 'https://www.amazon.com/b/ref=s9_acss_bw_cg_HarFLiv_2d1_w?node=3733261&pf_rd_m=ATVPDKIKX0DER&pf_rd_s=merchandised-search-2&pf_rd_r=MNGW2YC86GM462Z294K6&pf_rd_t=101&pf_rd_p=d8003c9c-74d3-490c-ae40-6aac6ed45f61&pf_rd_i=1063308',\n",
    "#     'Beds' : 'https://www.amazon.com/b/ref=s9_acss_bw_cg_HarFLiv_3a1_w?node=3248804011&pf_rd_m=ATVPDKIKX0DER&pf_rd_s=merchandised-search-2&pf_rd_r=MNGW2YC86GM462Z294K6&pf_rd_t=101&pf_rd_p=d8003c9c-74d3-490c-ae40-6aac6ed45f61&pf_rd_i=1063308',\n",
    "#     'Bedframes' : 'https://www.amazon.com/b/ref=s9_acss_bw_cg_HarFLiv_3b1_w?node=3248801011&pf_rd_m=ATVPDKIKX0DER&pf_rd_s=merchandised-search-2&pf_rd_r=MNGW2YC86GM462Z294K6&pf_rd_t=101&pf_rd_p=d8003c9c-74d3-490c-ae40-6aac6ed45f61&pf_rd_i=1063308',\n",
    "#     'Bases' : 'https://www.amazon.com/s?rh=n%3A17873917011&language=en_US&brr=1&pf_rd_i=1063308&pf_rd_m=ATVPDKIKX0DER&pf_rd_p=d8003c9c-74d3-490c-ae40-6aac6ed45f61&pf_rd_r=MNGW2YC86GM462Z294K6&pf_rd_s=merchandised-search-2&pf_rd_t=101&rd=1',\n",
    "#     'Vanities' : 'https://www.amazon.com/b/ref=s9_acss_bw_cg_HarFLiv_3d1_w?node=3733291&pf_rd_m=ATVPDKIKX0DER&pf_rd_s=merchandised-search-2&pf_rd_r=MNGW2YC86GM462Z294K6&pf_rd_t=101&pf_rd_p=d8003c9c-74d3-490c-ae40-6aac6ed45f61&pf_rd_i=1063308',    \n",
    "# }\n",
    "\n",
    "# Entryway_Furnitures = {\n",
    "#     'Entryway_Furnitures' : 'https://www.amazon.com/b/ref=s9_acss_bw_cg_FNNAV_5e1_w?node=3249856011&pf_rd_m=ATVPDKIKX0DER&pf_rd_s=merchandised-search-7&pf_rd_r=AMH2XC97ZGGNAWND0C03&pf_rd_t=101&pf_rd_p=d0c31dca-221f-4a26-9f62-a9e18ca9f354&pf_rd_i=1063306',\n",
    "# }\n",
    "\n",
    "# Home_Office_Furnitures = {\n",
    "#     'Desks' : 'https://www.amazon.com/b/ref=s9_acss_bw_cg_HarHOF_2a1_w?node=3733671&pf_rd_m=ATVPDKIKX0DER&pf_rd_s=merchandised-search-2&pf_rd_r=S6B16AQ2FYJDBJXK6X9C&pf_rd_t=101&pf_rd_p=b0e12b53-6018-44fe-b7a4-d1871cf08ee9&pf_rd_i=1063312',\n",
    "#     'Desk_Chairs' : 'https://www.amazon.com/b/ref=s9_acss_bw_cg_HarHOF_2b1_w?node=3733721&pf_rd_m=ATVPDKIKX0DER&pf_rd_s=merchandised-search-2&pf_rd_r=S6B16AQ2FYJDBJXK6X9C&pf_rd_t=101&pf_rd_p=b0e12b53-6018-44fe-b7a4-d1871cf08ee9&pf_rd_i=1063312',\n",
    "#     'Bookcases' : 'https://www.amazon.com/b/ref=s9_acss_bw_cg_HarHOF_2c1_w?node=10824421&pf_rd_m=ATVPDKIKX0DER&pf_rd_s=merchandised-search-2&pf_rd_r=S6B16AQ2FYJDBJXK6X9C&pf_rd_t=101&pf_rd_p=b0e12b53-6018-44fe-b7a4-d1871cf08ee9&pf_rd_i=1063312',\n",
    "#     'File_Cabinets' : 'https://www.amazon.com/b/ref=s9_acss_bw_cg_HarHOF_2d1_w?node=1069166&pf_rd_m=ATVPDKIKX0DER&pf_rd_s=merchandised-search-2&pf_rd_r=S6B16AQ2FYJDBJXK6X9C&pf_rd_t=101&pf_rd_p=b0e12b53-6018-44fe-b7a4-d1871cf08ee9&pf_rd_i=1063312',\n",
    "#     'Computer_Armoires' : 'https://www.amazon.com/s?rh=n%3A3733751&fs=true&ref=lp_3733751_sar',\n",
    "#     'Drafting_Tables' : 'https://www.amazon.com/s?rh=n%3A3733771&fs=true&ref=lp_3733771_sar',\n",
    "#     'Cabinets' : 'https://www.amazon.com/s?rh=n%3A3733761&fs=true&ref=lp_3733761_sar',\n",
    "#     'Furniture_Sets' : 'https://www.amazon.com/b/ref=s9_acss_bw_cg_HarHOF_3d1_w?node=3733661&pf_rd_m=ATVPDKIKX0DER&pf_rd_s=merchandised-search-2&pf_rd_r=S6B16AQ2FYJDBJXK6X9C&pf_rd_t=101&pf_rd_p=b0e12b53-6018-44fe-b7a4-d1871cf08ee9&pf_rd_i=1063312',\n",
    "# }\n",
    "\n",
    "dictionaries = [\n",
    "    Sofa_and_Couches, \n",
    "    # Other_Livingroom_Furniture, \n",
    "    # Decor_and_Soft_Furnishings, \n",
    "    # Kitchen_and_Dining_Furnitures,\n",
    "    # Bedroom_Furnitures, \n",
    "    # Entryway_Furnitures, \n",
    "    # Home_Office_Furnitures,\n",
    "]\n",
    "\n",
    "'-------------------------------------------------------------------------------'\n",
    "\n",
    "def scrape_infos(name, url):\n",
    "    \n",
    "    # initialize statue log txt file            \n",
    "    status_log_txt = 'crawling_status.txt'    \n",
    "    \n",
    "    # Start\n",
    "    next_operation = \"start\"\n",
    "    with open(status_log_txt, 'w', encoding='utf-8') as file:\n",
    "        file.write(next_operation + '\\n')\n",
    "        file.write(url + '\\n')\n",
    "        \n",
    "    # Each csv files\n",
    "    csv_file_path1 = f'page_urls/{name}_page_urls.csv'  \n",
    "    csv_file_path2 = f'product_urls/{name}_product_urls.csv'\n",
    "    csv_file_path3 = f'product_infos/{name}_product_infos.csv'\n",
    "    csv_file_path4 = f'error_urls/no_product_page_urls.csv'\n",
    "    csv_file_path5 = f'error_urls/no_info_product_urls.csv'\n",
    "    csv_file_path6 = f'error_urls/no_next_page_urls.csv'\n",
    "    csv_file_path7 = f'error_urls/no_img_product_urls.csv'\n",
    "    \n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs(f'page_urls', exist_ok=True)\n",
    "    os.makedirs(f'product_urls', exist_ok=True)\n",
    "    os.makedirs(f'product_infos', exist_ok=True)\n",
    "    os.makedirs(f'error_urls', exist_ok=True)\n",
    "    \n",
    "    # Headers for each csv\n",
    "    headers_needed1 = not os.path.exists(csv_file_path1)    \n",
    "    headers_needed2 = not os.path.exists(csv_file_path2)\n",
    "    headers_needed3 = not os.path.exists(csv_file_path3)\n",
    "    headers_needed4 = not os.path.exists(csv_file_path4)\n",
    "    headers_needed5 = not os.path.exists(csv_file_path5)\n",
    "    headers_needed6 = not os.path.exists(csv_file_path6)\n",
    "    headers_needed7 = not os.path.exists(csv_file_path7)\n",
    "    \n",
    "    if headers_needed1:\n",
    "        with open(csv_file_path1, mode='a', newline='', encoding='utf-8') as file1:\n",
    "            writer1 = csv.writer(file1)\n",
    "            writer1.writerow(['Page URL'])\n",
    "            file1.flush()\n",
    "    if headers_needed2:\n",
    "        with open(csv_file_path2, mode='a', newline='', encoding='utf-8') as file2:\n",
    "            writer2 = csv.writer(file2)\n",
    "            writer2.writerow(['Product URL'])\n",
    "            file2.flush()\n",
    "    if headers_needed3:\n",
    "        with open(csv_file_path3, mode='a', newline='', encoding='utf-8') as file3:\n",
    "            writer3 = csv.writer(file3)\n",
    "            writer3.writerow(['Category' , 'Title', 'Price', 'Product_Info', 'Product_Feature', 'Product_Text', 'Img_URL'])\n",
    "            file3.flush()  \n",
    "    if headers_needed4:\n",
    "        with open(csv_file_path4, mode='a', newline='', encoding='utf-8') as file4:\n",
    "            writer4 = csv.writer(file4)\n",
    "            writer4.writerow(['Error page URLs'])\n",
    "            file4.flush()  \n",
    "    if headers_needed5:\n",
    "        with open(csv_file_path5, mode='a', newline='', encoding='utf-8') as file5:\n",
    "            writer5 = csv.writer(file5)\n",
    "            writer5.writerow(['Error product URLs'])\n",
    "            file5.flush()          \n",
    "    if headers_needed6:\n",
    "        with open(csv_file_path6, mode='a', newline='', encoding='utf-8') as file6:\n",
    "            writer6 = csv.writer(file6)\n",
    "            writer6.writerow(['No next page URLs'])\n",
    "            file6.flush()          \n",
    "    if headers_needed7:\n",
    "        with open(csv_file_path7, mode='a', newline='', encoding='utf-8') as file7:\n",
    "            writer7 = csv.writer(file7)\n",
    "            writer7.writerow(['image get error product URLs'])\n",
    "            file7.flush() \n",
    "     \n",
    "    # 1. Open page\n",
    "    # 2. Get each product urls \n",
    "    # 3. Open each product urls, save infos, save img\n",
    "    # 4. Go to next page\n",
    "    # Repeat 1~4.\n",
    "    \n",
    "    # Configure driver\n",
    "    next_operation = 'configure driver'  \n",
    "    with open(status_log_txt, 'w', encoding='utf-8') as file:\n",
    "        file.write(next_operation + '\\n')\n",
    "        file.write(url + '\\n')\n",
    "    is_Configured, driver = configure_driver()\n",
    "    \n",
    "    # error in configuring driver\n",
    "    if is_Configured == False:\n",
    "        while is_Configured:\n",
    "            print('Re-configuring Driver...')\n",
    "            is_Configured, driver = configure_driver()  # configure driver over and over again\n",
    "             \n",
    "    # 1. Open page\n",
    "    next_operation = \"open page\"\n",
    "    with open(status_log_txt, 'w', encoding='utf-8') as file:\n",
    "        file.write(next_operation + '\\n')\n",
    "        file.write(url + '\\n')\n",
    "    is_PageOpened, driver = safe_get(driver, url) \n",
    "    \n",
    "    # error in opening first page\n",
    "    if is_PageOpened == False:\n",
    "        print(f\"Failed to load initial page: {url}\")\n",
    "        return  # end when Failed to load initial page\n",
    "    \n",
    "    # repeat 1~4. loop\n",
    "    is_Done = False\n",
    "    while not is_Done:  \n",
    "        # log current page url\n",
    "        current_page_url = driver.current_url\n",
    "        with open(csv_file_path1, mode='a', newline='', encoding='utf-8') as file1:  # page_urls/{name}_page_urls.csv\n",
    "            writer1 = csv.writer(file1)\n",
    "            writer1.writerow([current_page_url])\n",
    "            file1.flush()  # Force writing to CSV file\n",
    "            \n",
    "        next_page_url = get_nextpage_url(driver)  # returns None when fail to get next page\n",
    "    \n",
    "        # 2. get each product urls\n",
    "        next_operation = \"extract products url\"\n",
    "        with open(status_log_txt, 'w', encoding='utf-8') as file:\n",
    "            file.write(next_operation + '\\n')\n",
    "            file.write(current_page_url + '\\n')\n",
    "        product_urls = get_productpage_url(driver)  # returns [] when fail to get productpage urls / returns [[url1],[url2],...]\n",
    "        \n",
    "        # if no product on page (error)\n",
    "        if len(product_urls) == 0:\n",
    "            with open(csv_file_path4, mode='a', newline='', encoding='utf-8') as file4:  # no_product_page_urls.csv'\n",
    "                writer4 = csv.writer(file4)\n",
    "                writer4.writerow([current_page_url])\n",
    "                file4.flush()  \n",
    "            return  # end when fail to get product urls from list page\n",
    "        \n",
    "        # log product urls\n",
    "        with open(csv_file_path2, mode='a', newline='', encoding='utf-8') as file2:  # product_urls/{name}_product_urls.csv\n",
    "            writer2 = csv.writer(file2)   \n",
    "            writer2.writerows(product_urls)\n",
    "            file2.flush()     \n",
    "        \n",
    "        # 3. open each product urls, save infos\n",
    "        for [product_url] in product_urls:\n",
    "            # open each product page\n",
    "            next_operation =  'open product page'\n",
    "            with open(status_log_txt, 'w', encoding='utf-8') as file:\n",
    "                file.write(next_operation + '\\n')\n",
    "                file.write(current_page_url + '\\n')\n",
    "                file.write(product_url + '\\n')\n",
    "            is_ProductPageOpened, driver = safe_get(driver, product_url)\n",
    "\n",
    "            # error opening product page\n",
    "            if is_ProductPageOpened == False:\n",
    "                print(f\"Failed to load product page: {product_url}\")\n",
    "                # Handled later\n",
    "                # if ProductPageOpened == False, no product_info will be fetched. so it will be logged as fail to fetch info.\n",
    "            \n",
    "            # extract infos\n",
    "            next_operation = 'get infos'\n",
    "            with open(status_log_txt, 'w', encoding='utf-8') as file:\n",
    "                file.write(next_operation + '\\n')\n",
    "                file.write(current_page_url + '\\n')\n",
    "                file.write(product_url + '\\n')            \n",
    "            product_infos = fetch_info_from_product_page(driver)  # returns [] when fails to fetch infos\n",
    "\n",
    "            # product infos well extracted   \n",
    "            if len(product_infos) != 0:\n",
    "                # save product infos\n",
    "                next_operation = 'log infos'\n",
    "                with open(status_log_txt, 'w', encoding='utf-8') as file:\n",
    "                    file.write(next_operation + '\\n')\n",
    "                    file.write(current_page_url + '\\n')\n",
    "                    file.write(product_url + '\\n')\n",
    "                with open(csv_file_path3, mode='a', newline='', encoding='utf-8') as file3:  # product_infos/{name}_product_infos.csv\n",
    "                    writer3 = csv.writer(file3)\n",
    "                    writer3.writerow(product_infos)\n",
    "                    file3.flush()  # Force writing to CSV file\n",
    "                # save product img\n",
    "                title, image_url = product_infos[1], product_infos[-1]\n",
    "                next_operation = 'save img'\n",
    "                with open(status_log_txt, 'w', encoding='utf-8') as file:\n",
    "                    file.write(next_operation + '\\n')\n",
    "                    file.write(current_page_url + '\\n')\n",
    "                    file.write(image_url + '\\n')\n",
    "                is_ImgSaved = save_img(name, title, image_url)\n",
    "                \n",
    "                # error saving img\n",
    "                if is_ImgSaved == False:\n",
    "                    # log that product url for later retrying\n",
    "                    with open(csv_file_path7, mode='a', newline='', encoding='utf-8') as file7:  # error_urls/no_img_product_urls.csv\n",
    "                        writer7 = csv.writer(file7)\n",
    "                        writer7.writerow([product_url])\n",
    "                        file7.flush()\n",
    "\n",
    "            # product info extraction failed\n",
    "            else:\n",
    "                with open(csv_file_path5, mode='a', newline='', encoding='utf-8') as file5:  # error_urls/no_info_product_urls.csv\n",
    "                    writer5 = csv.writer(file5)\n",
    "                    writer5.writerow([product_url])\n",
    "                    file5.flush()  # Force writing to CSV file\n",
    "        \n",
    "        product_urls = []  # make product_urls list empty when iteration ended.\n",
    "        \n",
    "        # 4. go to next page\n",
    "        \n",
    "        # go back to list page\n",
    "        next_operation = 'go back to page'\n",
    "        with open(status_log_txt, 'w', encoding='utf-8') as file:\n",
    "            file.write(next_operation + '\\n')\n",
    "            file.write(current_page_url + '\\n')\n",
    "        is_BeforePageOpened, driver = safe_get(driver, current_page_url)\n",
    "        \n",
    "        # error opening before list page\n",
    "        if is_BeforePageOpened == False:\n",
    "            print(f\"Failed to load before page: {current_page_url}\")\n",
    "            # Handled later\n",
    "            # it will try to click next page, but will fail -> handled as is_NextPageClicked == False\n",
    "            \n",
    "        # click next page\n",
    "        next_operation = 'click next page'\n",
    "        with open(status_log_txt, 'w', encoding='utf-8') as file:\n",
    "            file.write(next_operation + '\\n')\n",
    "            file.write(current_page_url + '\\n')\n",
    "        is_NextPageClicked, driver = click_next_page(driver)\n",
    "\n",
    "        # error clicking NextPage\n",
    "        if is_NextPageClicked == False:\n",
    "            print(f'Failed to click next page: {current_page_url}')\n",
    "            with open(csv_file_path6, mode='a', newline='', encoding='utf-8') as file6:  # no_next_page_urls.csv\n",
    "                writer6 = csv.writer(file6)\n",
    "                writer6.writerow([current_page_url])\n",
    "                file6.flush()  # Force writing to CSV file\n",
    "                \n",
    "            # if there is next_page_url\n",
    "            if next_page_url:\n",
    "                # no \"next page\" icon to click, directly re-open next page with link\n",
    "                print('Trying to directly open next page')\n",
    "                next_operation = 'directly open next page'\n",
    "                with open(status_log_txt, 'w', encoding='utf-8') as file:\n",
    "                    file.write(next_operation + '\\n')\n",
    "                    file.write(current_page_url + '\\n')\n",
    "                # re-open the chrome\n",
    "                driver.quit()\n",
    "                is_Configured, driver = configure_driver()\n",
    "                \n",
    "                # error in configuring driver\n",
    "                if is_Configured == False:\n",
    "                    while is_Configured:\n",
    "                        print('Re-configuring Driver...')\n",
    "                        is_Configured, driver = configure_driver()  # configure driver over and over again\n",
    "            \n",
    "                # open next page directly\n",
    "                is_NextPageOpened, driver = safe_get(driver, next_page_url)\n",
    "                if is_NextPageOpened == False:\n",
    "                    print(f'Failed to diretly open next page: {current_page_url}')\n",
    "                    return  # end when Failed to load next page when there exist next page\n",
    "                else: \n",
    "                    print(f\"Success to open next page\")\n",
    "        \n",
    "            # no next_page_url and click_next_page fails == Done with current category\n",
    "            else:\n",
    "                is_Done = True\n",
    "            \n",
    "    # go to next category\n",
    "    next_operation = 'each start url finished'\n",
    "    with open(status_log_txt, 'w', encoding='utf-8') as file:\n",
    "        file.write(next_operation + '\\n')\n",
    "        file.write(current_page_url + '\\n')\n",
    "    print(f'{name} done. Moving to next category')\n",
    "\n",
    "'-------------------------------------------------------------------------------'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_Configured, driver = configure_driver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_log_txt = 'crawling_status.txt'\n",
    "last_operation = None\n",
    "\n",
    "# Check if status file exists and read the last operation and URL\n",
    "if os.path.exists(status_log_txt):\n",
    "    with open(status_log_txt, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "        if len(lines) >= 2:\n",
    "            last_operation = lines[0].strip()\n",
    "            last_url = lines[1].strip()\n",
    "\n",
    "    # last_operation = 'start', 'configure driver', \"open page\", \"extract products url\", 'open product page',\n",
    "    #                   'get infos', 'log infos', 'save img', 'go back to page', 'click next page',\n",
    "    #                   'each start url finished'\n",
    "\n",
    "if last_operation:  # if there is last_operation\n",
    "    url = last_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_2296\\2809088486.py:301: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  captcha_text = soup.find('h4', text=\"Type the characters you see in this image:\")\n"
     ]
    }
   ],
   "source": [
    "is_Success, driver = safe_get(driver, url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_urls = get_productpage_url(driver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.amazon.com/FULife-Convertible-Sectional-Sofa-Multi-Person/dp/B0C5XLYCFN/ref=sr_1_1297?fst=as%3Aoff&pf_rd_i=3733551%2C3733551&pf_rd_m=ATVPDKIKX0DER%2CATVPDKIKX0DER&pf_rd_p=877c0809-fc53-42a6-91f2-7c8f50e6e9be%2C877c0809-fc53-42a6-91f2-7c8f50e6e9be&pf_rd_r=0X2CMRE53H2HSQR09781%2C0X2CMRE53H2HSQR09781&pf_rd_s=merchandised-search-2%2Cmerchandised-search-2&pf_rd_t=101%2C101&qid=1702804208&refinements=p_n_feature_two_browse-bin%3A3248836011&rnid=3248834011%2C3248834011&s=home-garden&sr=1-1297\n"
     ]
    }
   ],
   "source": [
    "product_url = product_urls[0][0]\n",
    "print(product_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_2296\\2809088486.py:301: DeprecationWarning: The 'text' argument to find()-type methods is deprecated. Use 'string' instead.\n",
      "  captcha_text = soup.find('h4', text=\"Type the characters you see in this image:\")\n"
     ]
    }
   ],
   "source": [
    "is_Opened, driver = safe_get(driver, product_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_content = driver.page_source\n",
    "# Write the HTML content to a file\n",
    "with open('amazon_captcha.html', 'w', encoding='utf-8') as f:\n",
    "    f.write(html_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use BeautifulSoup to parse the HTML\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Find the captcha image\n",
    "captcha_image = soup.find('img')\n",
    "\n",
    "# Extract the 'src' attribute\n",
    "captcha_image_url = captcha_image['src'] if captcha_image else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://images-na.ssl-images-amazon.com/captcha/cdkxpfei/Captcha_okhxwwchqi.jpg\n"
     ]
    }
   ],
   "source": [
    "print(captcha_image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "captcha_text = soup.find('h4', string=\"Type the characters you see in this image:\")\n",
    "if captcha_text:\n",
    "    captcha_image = soup.find('img')\n",
    "    captcha_image_url = captcha_image['src'] if captcha_image else None\n",
    "    if captcha_image_url:\n",
    "        captcha = AmazonCaptcha.fromlink(captcha_image_url)\n",
    "        solution = captcha.solve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FKNFUE\n"
     ]
    }
   ],
   "source": [
    "print(solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.5.0\n"
     ]
    }
   ],
   "source": [
    "import PIL\n",
    "print(PIL.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the CAPTCHA input field by its ID\n",
    "captcha_input_field = driver.find_element(By.ID, \"captchacharacters\")\n",
    "\n",
    "# Send the CAPTCHA solution to the input field\n",
    "captcha_input_field.send_keys(solution)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit the form. This can be done in different ways:\n",
    "\n",
    "# Method 2: Send an Enter key press to the CAPTCHA input field\n",
    "captcha_input_field.send_keys(Keys.ENTER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "a = random.randint(0,1)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
