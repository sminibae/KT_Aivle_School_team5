{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-27 14:33:13.754929: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-12-27 14:33:15.432343: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-12-27 14:33:15.432378: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-12-27 14:33:15.625560: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-12-27 14:33:16.014207: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-27 14:33:18.647646: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # or '3' to suppress most of the logs\n",
    "tf.get_logger().setLevel('WARNING')  # Adjust logging level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('images64.npy')\n",
    "y = np.load('styles64.npy')\n",
    "# X and y are now numpy arrays containing your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Classic', 'Contemporary', 'Country', 'Minimalism', 'Modern',\n",
       "       'Unique', 'Urban'], dtype='<U12')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.calibration import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Assuming y contains integer labels\n",
    "y = to_categorical(y, num_classes=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "\n",
    "# Now proceed with the train_test_split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=1, stratify=y_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9400, 64, 64, 3) (1175, 64, 64, 3) (1175, 64, 64, 3)\n",
      "(9400, 7) (1175, 7) (1175, 7)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "print(y_train.shape, y_val.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 64, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, Dropout, Conv2D, AvgPool2D, Flatten, LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-27 14:33:46.289932: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-27 14:33:47.608158: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-27 14:33:47.608277: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-27 14:33:47.609262: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-27 14:33:47.609428: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-27 14:33:47.609555: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-27 14:33:47.759134: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-27 14:33:47.759254: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-27 14:33:47.759341: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-12-27 14:33:47.759400: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 12972 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 64, 64, 32)        1568      \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 64, 64, 32)        128       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 64, 64, 32)        0         \n",
      "                                                                 \n",
      " average_pooling2d (Average  (None, 16, 16, 32)        0         \n",
      " Pooling2D)                                                      \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 16, 16, 16)        8208      \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 16, 16, 16)        64        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 16, 16, 16)        0         \n",
      "                                                                 \n",
      " average_pooling2d_1 (Avera  (None, 4, 4, 16)          0         \n",
      " gePooling2D)                                                    \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 4, 4, 8)           2056      \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 4, 4, 8)           32        \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 4, 4, 8)           0         \n",
      "                                                                 \n",
      " average_pooling2d_2 (Avera  (None, 1, 1, 8)           0         \n",
      " gePooling2D)                                                    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 8)                 0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               1152      \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 128)               0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 7)                 903       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 7)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14623 (57.12 KB)\n",
      "Trainable params: 14255 (55.68 KB)\n",
      "Non-trainable params: 368 (1.44 KB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Input(X_train.shape[1:]),\n",
    "    \n",
    "    Conv2D(filters=32, kernel_size=(4,4), strides=(1,1), padding='same'),\n",
    "    BatchNormalization(),\n",
    "    LeakyReLU(alpha=0.01),\n",
    "    AvgPool2D(pool_size=(4,4)),\n",
    "    \n",
    "    Conv2D(filters=16, kernel_size=(4,4), strides=(1,1), padding='same'),\n",
    "    BatchNormalization(),\n",
    "    LeakyReLU(alpha=0.01),\n",
    "    AvgPool2D(pool_size=(4,4)),\n",
    "    \n",
    "    Conv2D(filters=8, kernel_size=(4,4), strides=(1,1), padding='same'),\n",
    "    BatchNormalization(),\n",
    "    LeakyReLU(alpha=0.01),\n",
    "    AvgPool2D(pool_size=(4,4)),\n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    Dense(128),\n",
    "    BatchNormalization(),\n",
    "    LeakyReLU(alpha=0.01),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    Dense(7),\n",
    "    Activation('softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                               patience=10,\n",
    "                               verbose=1,\n",
    "                               restore_best_weights=True)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss',\n",
    "                              factor=0.2,\n",
    "                              patience=5,\n",
    "                              verbose=1,\n",
    "                              min_lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "start fitting\n",
      "Epoch 1/10000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-27 14:33:50.988835: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8904\n",
      "2023-12-27 14:33:54.961023: I external/local_xla/xla/service/service.cc:168] XLA service 0x7fa4d1b97cd0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-12-27 14:33:54.961038: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce RTX 4090, Compute Capability 8.9\n",
      "2023-12-27 14:33:55.106995: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1703655235.272523   93079 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294/294 [==============================] - 9s 6ms/step - loss: 1.1444 - accuracy: 0.6666 - val_loss: 1.1394 - val_accuracy: 0.6970 - lr: 0.0010\n",
      "Epoch 2/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 1.0073 - accuracy: 0.7010 - val_loss: 1.4097 - val_accuracy: 0.5421 - lr: 0.0010\n",
      "Epoch 3/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.9606 - accuracy: 0.7078 - val_loss: 0.9774 - val_accuracy: 0.7098 - lr: 0.0010\n",
      "Epoch 4/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.9338 - accuracy: 0.7139 - val_loss: 1.3121 - val_accuracy: 0.6851 - lr: 0.0010\n",
      "Epoch 5/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.8965 - accuracy: 0.7248 - val_loss: 1.0946 - val_accuracy: 0.6562 - lr: 0.0010\n",
      "Epoch 6/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.8775 - accuracy: 0.7339 - val_loss: 1.1785 - val_accuracy: 0.6468 - lr: 0.0010\n",
      "Epoch 7/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.8624 - accuracy: 0.7341 - val_loss: 1.0982 - val_accuracy: 0.6851 - lr: 0.0010\n",
      "Epoch 8/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.8405 - accuracy: 0.7426 - val_loss: 1.0805 - val_accuracy: 0.6613 - lr: 0.0010\n",
      "Epoch 9/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.8273 - accuracy: 0.7406 - val_loss: 1.1818 - val_accuracy: 0.5847 - lr: 0.0010\n",
      "Epoch 10/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.8053 - accuracy: 0.7482 - val_loss: 0.8653 - val_accuracy: 0.7251 - lr: 0.0010\n",
      "Epoch 11/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.7980 - accuracy: 0.7511 - val_loss: 0.8663 - val_accuracy: 0.7523 - lr: 0.0010\n",
      "Epoch 12/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.7838 - accuracy: 0.7546 - val_loss: 1.0386 - val_accuracy: 0.7362 - lr: 0.0010\n",
      "Epoch 13/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.7690 - accuracy: 0.7594 - val_loss: 0.9068 - val_accuracy: 0.7098 - lr: 0.0010\n",
      "Epoch 14/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.7579 - accuracy: 0.7589 - val_loss: 0.9953 - val_accuracy: 0.7166 - lr: 0.0010\n",
      "Epoch 15/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.7422 - accuracy: 0.7627 - val_loss: 1.3877 - val_accuracy: 0.7038 - lr: 0.0010\n",
      "Epoch 16/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.7398 - accuracy: 0.7654 - val_loss: 0.8544 - val_accuracy: 0.7549 - lr: 0.0010\n",
      "Epoch 17/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.7215 - accuracy: 0.7740 - val_loss: 1.0935 - val_accuracy: 0.7260 - lr: 0.0010\n",
      "Epoch 18/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.7134 - accuracy: 0.7749 - val_loss: 0.9559 - val_accuracy: 0.7183 - lr: 0.0010\n",
      "Epoch 19/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.7036 - accuracy: 0.7779 - val_loss: 0.9996 - val_accuracy: 0.7260 - lr: 0.0010\n",
      "Epoch 20/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.6944 - accuracy: 0.7763 - val_loss: 1.0064 - val_accuracy: 0.7285 - lr: 0.0010\n",
      "Epoch 21/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.6739 - accuracy: 0.7837 - val_loss: 1.1926 - val_accuracy: 0.7166 - lr: 0.0010\n",
      "Epoch 22/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.6747 - accuracy: 0.7854 - val_loss: 1.0657 - val_accuracy: 0.6860 - lr: 0.0010\n",
      "Epoch 23/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.6597 - accuracy: 0.7928 - val_loss: 0.9668 - val_accuracy: 0.7438 - lr: 0.0010\n",
      "Epoch 24/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.6524 - accuracy: 0.7905 - val_loss: 1.0750 - val_accuracy: 0.6349 - lr: 0.0010\n",
      "Epoch 25/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.6377 - accuracy: 0.7962 - val_loss: 0.8746 - val_accuracy: 0.7277 - lr: 0.0010\n",
      "Epoch 26/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.6418 - accuracy: 0.7953 - val_loss: 0.8423 - val_accuracy: 0.7643 - lr: 0.0010\n",
      "Epoch 27/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.6335 - accuracy: 0.7982 - val_loss: 0.9550 - val_accuracy: 0.7302 - lr: 0.0010\n",
      "Epoch 28/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.6180 - accuracy: 0.7969 - val_loss: 0.9256 - val_accuracy: 0.7540 - lr: 0.0010\n",
      "Epoch 29/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.6155 - accuracy: 0.8004 - val_loss: 1.2755 - val_accuracy: 0.5506 - lr: 0.0010\n",
      "Epoch 30/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.6039 - accuracy: 0.8019 - val_loss: 0.8286 - val_accuracy: 0.7617 - lr: 0.0010\n",
      "Epoch 31/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.6039 - accuracy: 0.8038 - val_loss: 1.3036 - val_accuracy: 0.6809 - lr: 0.0010\n",
      "Epoch 32/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.5907 - accuracy: 0.8071 - val_loss: 0.8213 - val_accuracy: 0.7591 - lr: 0.0010\n",
      "Epoch 33/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.5846 - accuracy: 0.8093 - val_loss: 2.1066 - val_accuracy: 0.5370 - lr: 0.0010\n",
      "Epoch 34/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.5781 - accuracy: 0.8135 - val_loss: 0.7841 - val_accuracy: 0.7370 - lr: 0.0010\n",
      "Epoch 35/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.5788 - accuracy: 0.8099 - val_loss: 0.9575 - val_accuracy: 0.7319 - lr: 0.0010\n",
      "Epoch 36/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.5659 - accuracy: 0.8160 - val_loss: 0.8522 - val_accuracy: 0.7677 - lr: 0.0010\n",
      "Epoch 37/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.5647 - accuracy: 0.8118 - val_loss: 0.9000 - val_accuracy: 0.7694 - lr: 0.0010\n",
      "Epoch 38/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.5582 - accuracy: 0.8172 - val_loss: 0.7487 - val_accuracy: 0.7779 - lr: 0.0010\n",
      "Epoch 39/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.5556 - accuracy: 0.8213 - val_loss: 1.3214 - val_accuracy: 0.6417 - lr: 0.0010\n",
      "Epoch 40/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.5556 - accuracy: 0.8182 - val_loss: 0.7907 - val_accuracy: 0.7736 - lr: 0.0010\n",
      "Epoch 41/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.5399 - accuracy: 0.8216 - val_loss: 0.8996 - val_accuracy: 0.7626 - lr: 0.0010\n",
      "Epoch 42/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.5463 - accuracy: 0.8217 - val_loss: 0.7267 - val_accuracy: 0.7957 - lr: 0.0010\n",
      "Epoch 43/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.5338 - accuracy: 0.8240 - val_loss: 1.2665 - val_accuracy: 0.7200 - lr: 0.0010\n",
      "Epoch 44/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.5245 - accuracy: 0.8262 - val_loss: 1.0388 - val_accuracy: 0.6885 - lr: 0.0010\n",
      "Epoch 45/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.5209 - accuracy: 0.8281 - val_loss: 0.7584 - val_accuracy: 0.7855 - lr: 0.0010\n",
      "Epoch 46/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.5224 - accuracy: 0.8293 - val_loss: 0.7860 - val_accuracy: 0.7898 - lr: 0.0010\n",
      "Epoch 47/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.5122 - accuracy: 0.8288 - val_loss: 0.8685 - val_accuracy: 0.7132 - lr: 0.0010\n",
      "Epoch 48/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.5161 - accuracy: 0.8269 - val_loss: 1.0801 - val_accuracy: 0.7660 - lr: 0.0010\n",
      "Epoch 49/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.5085 - accuracy: 0.8317 - val_loss: 1.1794 - val_accuracy: 0.7396 - lr: 0.0010\n",
      "Epoch 50/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.4998 - accuracy: 0.8328 - val_loss: 1.3004 - val_accuracy: 0.7396 - lr: 0.0010\n",
      "Epoch 51/10000000\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.5016 - accuracy: 0.8315 - val_loss: 0.8645 - val_accuracy: 0.7336 - lr: 0.0010\n",
      "Epoch 52/10000000\n",
      "289/294 [============================>.] - ETA: 0s - loss: 0.4933 - accuracy: 0.8352Restoring model weights from the end of the best epoch: 42.\n",
      "294/294 [==============================] - 1s 3ms/step - loss: 0.4925 - accuracy: 0.8355 - val_loss: 0.8681 - val_accuracy: 0.7702 - lr: 0.0010\n",
      "Epoch 52: early stopping\n",
      "fitting done\n"
     ]
    }
   ],
   "source": [
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "print('start fitting')\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10000000,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val,y_val),\n",
    "    callbacks=[early_stopping, reduce_lr],\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "print('fitting done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 0s 1ms/step\n",
      "[1.0960133e-02 1.0858343e-02 1.2964699e-01 9.6850577e-05 7.1801579e-01\n",
      " 4.8754831e-05 1.3037305e-01]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([4, 4, 4, 4, 2, 4, 2, 4, 4, 4])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(y_pred[0])\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "y_pred_labels[:10]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7838297872340425\n",
      "Confusion Matrix:\n",
      " [[ 54   9   4   0  60   0   1]\n",
      " [  3  22   5   1  56   0   0]\n",
      " [  4   4  43   0  22   0   0]\n",
      " [  1   0   0   4  11   0   0]\n",
      " [ 15  12  14   0 768   0   9]\n",
      " [  0   0   0   0   4   3   0]\n",
      " [  0   2   1   0  16   0  27]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.42      0.53       128\n",
      "           1       0.45      0.25      0.32        87\n",
      "           2       0.64      0.59      0.61        73\n",
      "           3       0.80      0.25      0.38        16\n",
      "           4       0.82      0.94      0.88       818\n",
      "           5       1.00      0.43      0.60         7\n",
      "           6       0.73      0.59      0.65        46\n",
      "\n",
      "    accuracy                           0.78      1175\n",
      "   macro avg       0.73      0.50      0.57      1175\n",
      "weighted avg       0.77      0.78      0.76      1175\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if y_test.ndim > 1:  # Check if y_test is one-hot encoded\n",
    "    y_test_int_labels = np.argmax(y_test, axis=1)\n",
    "else:\n",
    "    y_test_int_labels = y_test  # y_test is already in integer label format\n",
    "\n",
    "# Now calculate the accuracy\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test_int_labels, y_pred_labels))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test_int_labels, y_pred_labels))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test_int_labels, y_pred_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('images64.npy')\n",
    "y = np.load('styles64.npy')\n",
    "# X and y are now numpy arrays containing your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9400, 12288) (1175, 12288) (1175, 12288)\n",
      "(9400,) (1175,) (1175,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.calibration import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# Reshape X from (len(X), 64, 64, 3) to (len(X), 1, 12288)\n",
    "X = X.reshape(len(X),-1)  # -1 tells numpy to infer the dimension\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Now proceed with the train_test_split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=1, stratify=y_temp)\n",
    "\n",
    "print(X_train.shape, X_val.shape, X_test.shape)\n",
    "print(y_train.shape, y_val.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "import xgboost as xgb\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.91%\n",
      "Precision: 0.89\n",
      "Recall: 0.88\n",
      "F1 Score: 0.87\n",
      "Confusion Matrix:\n",
      "[[ 72   0   0   0  56   0   0]\n",
      " [  0  47   0   0  40   0   0]\n",
      " [  1   1  53   0  19   0   0]\n",
      " [  1   0   0  12   3   0   0]\n",
      " [  5   0   0   0 814   0   0]\n",
      " [  0   0   0   0   3   3   0]\n",
      " [  0   0   0   0  13   0  32]]\n"
     ]
    }
   ],
   "source": [
    "# Random Forest\n",
    "RFC = RandomForestClassifier()\n",
    "RFC.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_RFC = RFC.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "accuracy_RFC = accuracy_score(y_test, y_pred_RFC)\n",
    "print(f\"Accuracy: {accuracy_RFC * 100:.2f}%\")\n",
    "\n",
    "# Precision, Recall, and F1 Score\n",
    "precision_RFC = precision_score(y_test, y_pred_RFC, average='weighted')\n",
    "recall_RFC = recall_score(y_test, y_pred_RFC, average='weighted')\n",
    "f1_RFC = f1_score(y_test, y_pred_RFC, average='weighted')\n",
    "\n",
    "print(f\"Precision: {precision_RFC:.2f}\")\n",
    "print(f\"Recall: {recall_RFC:.2f}\")\n",
    "print(f\"F1 Score: {f1_RFC:.2f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix_RFC = confusion_matrix(y_test, y_pred_RFC)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_RFC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.26%\n",
      "Precision: 0.90\n",
      "Recall: 0.88\n",
      "F1 Score: 0.87\n",
      "Confusion Matrix:\n",
      "[[ 74   0   0   0  54   0   0]\n",
      " [  0  47   0   0  40   0   0]\n",
      " [  1   0  53   0  20   0   0]\n",
      " [  0   0   0  12   4   0   0]\n",
      " [  3   0   0   0 816   0   0]\n",
      " [  0   0   0   0   3   3   0]\n",
      " [  0   0   0   0  13   0  32]]\n"
     ]
    }
   ],
   "source": [
    "# SVM Classifier\n",
    "SVM = SVC()\n",
    "\n",
    "# Random Forest\n",
    "SVM = RandomForestClassifier()\n",
    "SVM.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_SVM = SVM.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "accuracy_SVM = accuracy_score(y_test, y_pred_SVM)\n",
    "print(f\"Accuracy: {accuracy_SVM * 100:.2f}%\")\n",
    "\n",
    "# Precision, Recall, and F1 Score\n",
    "precision_SVM = precision_score(y_test, y_pred_SVM, average='weighted')\n",
    "recall_SVM = recall_score(y_test, y_pred_SVM, average='weighted')\n",
    "f1_SVM = f1_score(y_test, y_pred_SVM, average='weighted')\n",
    "\n",
    "print(f\"Precision: {precision_SVM:.2f}\")\n",
    "print(f\"Recall: {recall_SVM:.2f}\")\n",
    "print(f\"F1 Score: {f1_SVM:.2f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix_SVM = confusion_matrix(y_test, y_pred_SVM)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_SVM)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['svm_model.joblib']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Assuming your SVM model is named as SVM\n",
    "joblib.dump(SVM, 'svm_model.joblib')\n",
    "\n",
    "# # Load the model from the file\n",
    "# SVM_model = joblib.load('svm_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.83%\n",
      "Precision: 0.89\n",
      "Recall: 0.88\n",
      "F1 Score: 0.87\n",
      "Confusion Matrix:\n",
      "[[ 75   0   0   0  53   0   0]\n",
      " [  0  47   0   1  39   0   0]\n",
      " [  1   0  53   0  19   0   1]\n",
      " [  1   0   0  12   3   0   0]\n",
      " [  7   1   1   0 810   0   0]\n",
      " [  0   0   0   0   3   3   0]\n",
      " [  0   0   0   0  13   0  32]]\n"
     ]
    }
   ],
   "source": [
    "# XGBClassifier\n",
    "XGB= xgb.XGBClassifier()\n",
    "\n",
    "XGB.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_XGB = XGB.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "accuracy_XGB = accuracy_score(y_test, y_pred_XGB)\n",
    "print(f\"Accuracy: {accuracy_XGB * 100:.2f}%\")\n",
    "\n",
    "# Precision, Recall, and F1 Score\n",
    "precision_XGB = precision_score(y_test, y_pred_XGB, average='weighted')\n",
    "recall_XGB = recall_score(y_test, y_pred_XGB, average='weighted')\n",
    "f1_XGB = f1_score(y_test, y_pred_XGB, average='weighted')\n",
    "\n",
    "print(f\"Precision: {precision_XGB:.2f}\")\n",
    "print(f\"Recall: {recall_XGB:.2f}\")\n",
    "print(f\"F1 Score: {f1_XGB:.2f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix_XGB = confusion_matrix(y_test, y_pred_XGB)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_XGB)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 27.40%\n",
      "Precision: 0.62\n",
      "Recall: 0.27\n",
      "F1 Score: 0.34\n",
      "Confusion Matrix:\n",
      "[[ 22  23   4  10  19  11  39]\n",
      " [ 13  21   1  12  18   9  13]\n",
      " [  7  16  20   3   9   3  16]\n",
      " [  1   3   0   3   0   1   8]\n",
      " [ 71  98  51 103 223 108 165]\n",
      " [  0   0   0   0   0   3   3]\n",
      " [  5   2   1   2   4   1  30]]\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes\n",
    "GNB= GaussianNB()\n",
    "\n",
    "GNB.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_GNB = GNB.predict(X_test)\n",
    "\n",
    "# Accuracy\n",
    "accuracy_GNB = accuracy_score(y_test, y_pred_GNB)\n",
    "print(f\"Accuracy: {accuracy_GNB * 100:.2f}%\")\n",
    "\n",
    "# Precision, Recall, and F1 Score\n",
    "precision_GNB = precision_score(y_test, y_pred_GNB, average='weighted')\n",
    "recall_GNB = recall_score(y_test, y_pred_GNB, average='weighted')\n",
    "f1_GNB = f1_score(y_test, y_pred_GNB, average='weighted')\n",
    "\n",
    "print(f\"Precision: {precision_GNB:.2f}\")\n",
    "print(f\"Recall: {recall_GNB:.2f}\")\n",
    "print(f\"F1 Score: {f1_GNB:.2f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix_GNB = confusion_matrix(y_test, y_pred_GNB)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix_GNB)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
